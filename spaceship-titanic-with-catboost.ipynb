{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44dac8c9",
   "metadata": {
    "id": "Ck00s7mTmnjA",
    "papermill": {
     "duration": 0.012573,
     "end_time": "2023-08-05T15:14:53.488101",
     "exception": false,
     "start_time": "2023-08-05T15:14:53.475528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Spaceship Titanic Transportation Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0208cb1",
   "metadata": {
    "id": "UPNzfVOEmnjH",
    "papermill": {
     "duration": 0.010957,
     "end_time": "2023-08-05T15:14:53.510628",
     "exception": false,
     "start_time": "2023-08-05T15:14:53.499671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4acdf91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:14:53.535909Z",
     "iopub.status.busy": "2023-08-05T15:14:53.534931Z",
     "iopub.status.idle": "2023-08-05T15:15:05.451445Z",
     "shell.execute_reply": "2023-08-05T15:15:05.449870Z"
    },
    "id": "mmwBzpblmnjH",
    "papermill": {
     "duration": 11.933128,
     "end_time": "2023-08-05T15:15:05.455122",
     "exception": false,
     "start_time": "2023-08-05T15:14:53.521994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609897f2",
   "metadata": {
    "id": "6sHFpppPmnjJ",
    "papermill": {
     "duration": 0.011185,
     "end_time": "2023-08-05T15:15:05.478277",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.467092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be223b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:05.505004Z",
     "iopub.status.busy": "2023-08-05T15:15:05.503170Z",
     "iopub.status.idle": "2023-08-05T15:15:05.571022Z",
     "shell.execute_reply": "2023-08-05T15:15:05.569939Z"
    },
    "id": "c1P3Y3a7mnjL",
    "papermill": {
     "duration": 0.083931,
     "end_time": "2023-08-05T15:15:05.573892",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.489961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train dataset shape is (8693, 14)\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset into a Pandas Dataframe\n",
    "df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\n",
    "print(\"Full train dataset shape is {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8f5ae1",
   "metadata": {
    "id": "cEd92zhJmnjL",
    "papermill": {
     "duration": 0.01147,
     "end_time": "2023-08-05T15:15:05.597203",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.585733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The data is composed of 14 columns and 8693 entries. We can see all 14 dimensions of our dataset by printing out the first 5 entries using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0c9dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:05.624157Z",
     "iopub.status.busy": "2023-08-05T15:15:05.623276Z",
     "iopub.status.idle": "2023-08-05T15:15:05.664941Z",
     "shell.execute_reply": "2023-08-05T15:15:05.663556Z"
    },
    "id": "nCx3PE1xmnjM",
    "papermill": {
     "duration": 0.058434,
     "end_time": "2023-08-05T15:15:05.667889",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.609455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "\n",
       "   Transported  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 examples\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00007c",
   "metadata": {
    "id": "0-Euaq6dmnjN",
    "papermill": {
     "duration": 0.011792,
     "end_time": "2023-08-05T15:15:05.692964",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.681172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 12 feature columns. Using these features your model has to predict whether the passenger is rescued or not indicated by the column `Transported`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f4134",
   "metadata": {
    "id": "1-Ewr6XDmnjN",
    "papermill": {
     "duration": 0.011793,
     "end_time": "2023-08-05T15:15:05.716995",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.705202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Let us quickly do a basic exploration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bb38f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:05.744296Z",
     "iopub.status.busy": "2023-08-05T15:15:05.743793Z",
     "iopub.status.idle": "2023-08-05T15:15:05.791855Z",
     "shell.execute_reply": "2023-08-05T15:15:05.790449Z"
    },
    "id": "XjwG5wjfmnjO",
    "papermill": {
     "duration": 0.065125,
     "end_time": "2023-08-05T15:15:05.794621",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.729496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8514.000000</td>\n",
       "      <td>8512.000000</td>\n",
       "      <td>8510.000000</td>\n",
       "      <td>8485.000000</td>\n",
       "      <td>8510.000000</td>\n",
       "      <td>8505.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.827930</td>\n",
       "      <td>224.687617</td>\n",
       "      <td>458.077203</td>\n",
       "      <td>173.729169</td>\n",
       "      <td>311.138778</td>\n",
       "      <td>304.854791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.489021</td>\n",
       "      <td>666.717663</td>\n",
       "      <td>1611.489240</td>\n",
       "      <td>604.696458</td>\n",
       "      <td>1136.705535</td>\n",
       "      <td>1145.717189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>14327.000000</td>\n",
       "      <td>29813.000000</td>\n",
       "      <td>23492.000000</td>\n",
       "      <td>22408.000000</td>\n",
       "      <td>24133.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age   RoomService     FoodCourt  ShoppingMall           Spa  \\\n",
       "count  8514.000000   8512.000000   8510.000000   8485.000000   8510.000000   \n",
       "mean     28.827930    224.687617    458.077203    173.729169    311.138778   \n",
       "std      14.489021    666.717663   1611.489240    604.696458   1136.705535   \n",
       "min       0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      19.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%      27.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%      38.000000     47.000000     76.000000     27.000000     59.000000   \n",
       "max      79.000000  14327.000000  29813.000000  23492.000000  22408.000000   \n",
       "\n",
       "             VRDeck  \n",
       "count   8505.000000  \n",
       "mean     304.854791  \n",
       "std     1145.717189  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%       46.000000  \n",
       "max    24133.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426ee93d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:05.822268Z",
     "iopub.status.busy": "2023-08-05T15:15:05.821408Z",
     "iopub.status.idle": "2023-08-05T15:15:05.849476Z",
     "shell.execute_reply": "2023-08-05T15:15:05.847869Z"
    },
    "id": "UmWpnVxQmnjO",
    "papermill": {
     "duration": 0.044767,
     "end_time": "2023-08-05T15:15:05.852177",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.807410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b5b3e",
   "metadata": {
    "id": "PYbIPVaCmnjO",
    "papermill": {
     "duration": 0.012337,
     "end_time": "2023-08-05T15:15:05.877363",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.865026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bar chart for label column: Transported\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f367c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:05.905476Z",
     "iopub.status.busy": "2023-08-05T15:15:05.904587Z",
     "iopub.status.idle": "2023-08-05T15:15:06.399950Z",
     "shell.execute_reply": "2023-08-05T15:15:06.398590Z"
    },
    "id": "DcaGweARmnjP",
    "papermill": {
     "duration": 0.512624,
     "end_time": "2023-08-05T15:15:06.402758",
     "exception": false,
     "start_time": "2023-08-05T15:15:05.890134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = df.Transported.value_counts()\n",
    "plot_df.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9a46c",
   "metadata": {
    "id": "LRO2hJlNmnjP",
    "papermill": {
     "duration": 0.012827,
     "end_time": "2023-08-05T15:15:06.428984",
     "exception": false,
     "start_time": "2023-08-05T15:15:06.416157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Numerical data distribution\n",
    "\n",
    "Let us plot all the numerical columns and their value counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4680bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:06.457700Z",
     "iopub.status.busy": "2023-08-05T15:15:06.457200Z",
     "iopub.status.idle": "2023-08-05T15:15:08.114829Z",
     "shell.execute_reply": "2023-08-05T15:15:08.113665Z"
    },
    "id": "lafxj4fkmnjQ",
    "papermill": {
     "duration": 1.675194,
     "end_time": "2023-08-05T15:15:08.117584",
     "exception": false,
     "start_time": "2023-08-05T15:15:06.442390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(5,1,  figsize=(10, 10))\n",
    "plt.subplots_adjust(top = 2)\n",
    "\n",
    "sns.histplot(df['Age'], color='b', bins=50, ax=ax[0]);\n",
    "sns.histplot(df['FoodCourt'], color='b', bins=50, ax=ax[1]);\n",
    "sns.histplot(df['ShoppingMall'], color='b', bins=50, ax=ax[2]);\n",
    "sns.histplot(df['Spa'], color='b', bins=50, ax=ax[3]);\n",
    "sns.histplot(df['VRDeck'], color='b', bins=50, ax=ax[4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec0044",
   "metadata": {
    "id": "Trlsxv1emnjQ",
    "papermill": {
     "duration": 0.014498,
     "end_time": "2023-08-05T15:15:08.147445",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.132947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce80461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.180391Z",
     "iopub.status.busy": "2023-08-05T15:15:08.178873Z",
     "iopub.status.idle": "2023-08-05T15:15:08.189717Z",
     "shell.execute_reply": "2023-08-05T15:15:08.188294Z"
    },
    "papermill": {
     "duration": 0.03015,
     "end_time": "2023-08-05T15:15:08.192705",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.162555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      object\n",
       "HomePlanet       object\n",
       "CryoSleep        object\n",
       "Cabin            object\n",
       "Destination      object\n",
       "Age             float64\n",
       "VIP              object\n",
       "RoomService     float64\n",
       "FoodCourt       float64\n",
       "ShoppingMall    float64\n",
       "Spa             float64\n",
       "VRDeck          float64\n",
       "Name             object\n",
       "Transported        bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b493cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.224788Z",
     "iopub.status.busy": "2023-08-05T15:15:08.224305Z",
     "iopub.status.idle": "2023-08-05T15:15:08.233803Z",
     "shell.execute_reply": "2023-08-05T15:15:08.232626Z"
    },
    "papermill": {
     "duration": 0.029105,
     "end_time": "2023-08-05T15:15:08.236949",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.207844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(['PassengerId', 'Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8a5d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.269414Z",
     "iopub.status.busy": "2023-08-05T15:15:08.268897Z",
     "iopub.status.idle": "2023-08-05T15:15:08.327303Z",
     "shell.execute_reply": "2023-08-05T15:15:08.325888Z"
    },
    "papermill": {
     "duration": 0.078209,
     "end_time": "2023-08-05T15:15:08.330362",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.252153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/98/P</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>41.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>G/1499/S</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/1500/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>32.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>44.0</td>\n",
       "      <td>False</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7788 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HomePlanet CryoSleep     Cabin    Destination   Age    VIP  RoomService  \\\n",
       "0        Europa     False     B/0/P    TRAPPIST-1e  39.0  False          0.0   \n",
       "1         Earth     False     F/0/S    TRAPPIST-1e  24.0  False        109.0   \n",
       "2        Europa     False     A/0/S    TRAPPIST-1e  58.0   True         43.0   \n",
       "3        Europa     False     A/0/S    TRAPPIST-1e  33.0  False          0.0   \n",
       "4         Earth     False     F/1/S    TRAPPIST-1e  16.0  False        303.0   \n",
       "...         ...       ...       ...            ...   ...    ...          ...   \n",
       "8688     Europa     False    A/98/P    55 Cancri e  41.0   True          0.0   \n",
       "8689      Earth      True  G/1499/S  PSO J318.5-22  18.0  False          0.0   \n",
       "8690      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False          0.0   \n",
       "8691     Europa     False   E/608/S    55 Cancri e  32.0  False          0.0   \n",
       "8692     Europa     False   E/608/S    TRAPPIST-1e  44.0  False        126.0   \n",
       "\n",
       "      FoodCourt  ShoppingMall     Spa  VRDeck  Transported  \n",
       "0           0.0           0.0     0.0     0.0        False  \n",
       "1           9.0          25.0   549.0    44.0         True  \n",
       "2        3576.0           0.0  6715.0    49.0        False  \n",
       "3        1283.0         371.0  3329.0   193.0        False  \n",
       "4          70.0         151.0   565.0     2.0         True  \n",
       "...         ...           ...     ...     ...          ...  \n",
       "8688     6819.0           0.0  1643.0    74.0        False  \n",
       "8689        0.0           0.0     0.0     0.0        False  \n",
       "8690        0.0        1872.0     1.0     0.0         True  \n",
       "8691     1049.0           0.0   353.0  3235.0        False  \n",
       "8692     4688.0           0.0     0.0    12.0         True  \n",
       "\n",
       "[7788 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].fillna(value=0)\n",
    "df.isnull().sum().sort_values(ascending=False)\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974f4f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.364346Z",
     "iopub.status.busy": "2023-08-05T15:15:08.363894Z",
     "iopub.status.idle": "2023-08-05T15:15:08.387239Z",
     "shell.execute_reply": "2023-08-05T15:15:08.385864Z"
    },
    "papermill": {
     "duration": 0.043654,
     "end_time": "2023-08-05T15:15:08.390002",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.346348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HomePlanet CryoSleep  Cabin  Destination   Age    VIP  RoomService  \\\n",
       "0     Europa     False  B/0/P  TRAPPIST-1e  39.0  False          0.0   \n",
       "1      Earth     False  F/0/S  TRAPPIST-1e  24.0  False        109.0   \n",
       "2     Europa     False  A/0/S  TRAPPIST-1e  58.0   True         43.0   \n",
       "3     Europa     False  A/0/S  TRAPPIST-1e  33.0  False          0.0   \n",
       "4      Earth     False  F/1/S  TRAPPIST-1e  16.0  False        303.0   \n",
       "\n",
       "   FoodCourt  ShoppingMall     Spa  VRDeck  Transported  \n",
       "0        0.0           0.0     0.0     0.0        False  \n",
       "1        9.0          25.0   549.0    44.0         True  \n",
       "2     3576.0           0.0  6715.0    49.0        False  \n",
       "3     1283.0         371.0  3329.0   193.0        False  \n",
       "4       70.0         151.0   565.0     2.0         True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2541cd62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.425504Z",
     "iopub.status.busy": "2023-08-05T15:15:08.424091Z",
     "iopub.status.idle": "2023-08-05T15:15:08.455465Z",
     "shell.execute_reply": "2023-08-05T15:15:08.454162Z"
    },
    "papermill": {
     "duration": 0.052404,
     "end_time": "2023-08-05T15:15:08.458806",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.406402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"Deck\", \"Cabin_num\", \"Side\"]] = df[\"Cabin\"].str.split(\"/\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2446c50e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.493658Z",
     "iopub.status.busy": "2023-08-05T15:15:08.493195Z",
     "iopub.status.idle": "2023-08-05T15:15:08.533640Z",
     "shell.execute_reply": "2023-08-05T15:15:08.532336Z"
    },
    "papermill": {
     "duration": 0.061073,
     "end_time": "2023-08-05T15:15:08.536629",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.475556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>Deck</th>\n",
       "      <th>Cabin_num</th>\n",
       "      <th>Side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>True</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HomePlanet CryoSleep  Cabin  Destination   Age    VIP  RoomService  \\\n",
       "0     Europa     False  B/0/P  TRAPPIST-1e  39.0  False          0.0   \n",
       "1      Earth     False  F/0/S  TRAPPIST-1e  24.0  False        109.0   \n",
       "2     Europa     False  A/0/S  TRAPPIST-1e  58.0   True         43.0   \n",
       "3     Europa     False  A/0/S  TRAPPIST-1e  33.0  False          0.0   \n",
       "4      Earth     False  F/1/S  TRAPPIST-1e  16.0  False        303.0   \n",
       "\n",
       "   FoodCourt  ShoppingMall     Spa  VRDeck  Transported Deck Cabin_num Side  \n",
       "0        0.0           0.0     0.0     0.0        False    B         0    P  \n",
       "1        9.0          25.0   549.0    44.0         True    F         0    S  \n",
       "2     3576.0           0.0  6715.0    49.0        False    A         0    S  \n",
       "3     1283.0         371.0  3329.0   193.0        False    A         0    S  \n",
       "4       70.0         151.0   565.0     2.0         True    F         1    S  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"HomePlanet\"] = df[\"HomePlanet\"].astype('category')\n",
    "df[\"Destination\"] = df[\"Destination\"].astype('category')\n",
    "df[\"Deck\"] = df[\"Deck\"].astype('category')\n",
    "df[\"Side\"] = df[\"Side\"].astype('category')\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af354ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.571870Z",
     "iopub.status.busy": "2023-08-05T15:15:08.571410Z",
     "iopub.status.idle": "2023-08-05T15:15:08.581412Z",
     "shell.execute_reply": "2023-08-05T15:15:08.580165Z"
    },
    "papermill": {
     "duration": 0.03088,
     "end_time": "2023-08-05T15:15:08.584218",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.553338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"hp_cat\"] = df[\"HomePlanet\"].cat.codes\n",
    "df[\"dest_cat\"] = df[\"Destination\"].cat.codes\n",
    "df[\"deck_cat\"] = df[\"Deck\"].cat.codes\n",
    "df[\"side_cat\"] = df[\"Side\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83177563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.619948Z",
     "iopub.status.busy": "2023-08-05T15:15:08.618908Z",
     "iopub.status.idle": "2023-08-05T15:15:08.629132Z",
     "shell.execute_reply": "2023-08-05T15:15:08.627967Z"
    },
    "papermill": {
     "duration": 0.03144,
     "end_time": "2023-08-05T15:15:08.632339",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.600899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['VIP'] = df['VIP'].astype(int)\n",
    "df['CryoSleep'] = df['CryoSleep'].astype(int)\n",
    "# obj_df['Cabin_num'] = obj_df['Cabin_num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8064ea71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.669202Z",
     "iopub.status.busy": "2023-08-05T15:15:08.667642Z",
     "iopub.status.idle": "2023-08-05T15:15:08.679755Z",
     "shell.execute_reply": "2023-08-05T15:15:08.678092Z"
    },
    "papermill": {
     "duration": 0.033626,
     "end_time": "2023-08-05T15:15:08.682816",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.649190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.drop(['Cabin_num','HomePlanet', 'Cabin', 'Destination', 'Deck', 'Side'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1347c393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.719659Z",
     "iopub.status.busy": "2023-08-05T15:15:08.719180Z",
     "iopub.status.idle": "2023-08-05T15:15:08.725661Z",
     "shell.execute_reply": "2023-08-05T15:15:08.724084Z"
    },
    "papermill": {
     "duration": 0.028455,
     "end_time": "2023-08-05T15:15:08.728640",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.700185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Transported'] = df['Transported'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c405f6f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.766071Z",
     "iopub.status.busy": "2023-08-05T15:15:08.765528Z",
     "iopub.status.idle": "2023-08-05T15:15:08.775470Z",
     "shell.execute_reply": "2023-08-05T15:15:08.774344Z"
    },
    "papermill": {
     "duration": 0.032754,
     "end_time": "2023-08-05T15:15:08.778350",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.745596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CryoSleep         int64\n",
       "Age             float64\n",
       "VIP               int64\n",
       "RoomService     float64\n",
       "FoodCourt       float64\n",
       "ShoppingMall    float64\n",
       "Spa             float64\n",
       "VRDeck          float64\n",
       "Transported       int64\n",
       "hp_cat             int8\n",
       "dest_cat           int8\n",
       "deck_cat           int8\n",
       "side_cat           int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71c4d5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.815506Z",
     "iopub.status.busy": "2023-08-05T15:15:08.814226Z",
     "iopub.status.idle": "2023-08-05T15:15:08.823035Z",
     "shell.execute_reply": "2023-08-05T15:15:08.821894Z"
    },
    "papermill": {
     "duration": 0.030303,
     "end_time": "2023-08-05T15:15:08.825885",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.795582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[['hp_cat', 'CryoSleep', 'dest_cat', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'deck_cat', 'side_cat', 'Transported']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c5b68",
   "metadata": {
    "papermill": {
     "duration": 0.017027,
     "end_time": "2023-08-05T15:15:08.860508",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.843481",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d51eb1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.898628Z",
     "iopub.status.busy": "2023-08-05T15:15:08.898135Z",
     "iopub.status.idle": "2023-08-05T15:15:08.922116Z",
     "shell.execute_reply": "2023-08-05T15:15:08.920544Z"
    },
    "id": "g0U1OKAYmnjW",
    "papermill": {
     "duration": 0.046316,
     "end_time": "2023-08-05T15:15:08.925101",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.878785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp_cat</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>dest_cat</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>deck_cat</th>\n",
       "      <th>side_cat</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hp_cat  CryoSleep  dest_cat   Age  VIP  RoomService  FoodCourt  \\\n",
       "0       1          0         2  39.0    0          0.0        0.0   \n",
       "1       0          0         2  24.0    0        109.0        9.0   \n",
       "2       1          0         2  58.0    1         43.0     3576.0   \n",
       "3       1          0         2  33.0    0          0.0     1283.0   \n",
       "4       0          0         2  16.0    0        303.0       70.0   \n",
       "\n",
       "   ShoppingMall     Spa  VRDeck  deck_cat  side_cat  Transported  \n",
       "0           0.0     0.0     0.0         1         0            0  \n",
       "1          25.0   549.0    44.0         5         1            1  \n",
       "2           0.0  6715.0    49.0         0         1            0  \n",
       "3         371.0  3329.0   193.0         0         1            0  \n",
       "4         151.0   565.0     2.0         5         1            1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773cbc03",
   "metadata": {
    "id": "l6EmJsSfmnjW",
    "papermill": {
     "duration": 0.018021,
     "end_time": "2023-08-05T15:15:08.961043",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.943022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let us split the dataset into training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d494bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:08.998911Z",
     "iopub.status.busy": "2023-08-05T15:15:08.998456Z",
     "iopub.status.idle": "2023-08-05T15:15:09.010313Z",
     "shell.execute_reply": "2023-08-05T15:15:09.008676Z"
    },
    "id": "_drMgwAAmnjX",
    "papermill": {
     "duration": 0.034223,
     "end_time": "2023-08-05T15:15:09.013554",
     "exception": false,
     "start_time": "2023-08-05T15:15:08.979331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a635bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:09.051532Z",
     "iopub.status.busy": "2023-08-05T15:15:09.051071Z",
     "iopub.status.idle": "2023-08-05T15:15:09.062075Z",
     "shell.execute_reply": "2023-08-05T15:15:09.061116Z"
    },
    "papermill": {
     "duration": 0.033377,
     "end_time": "2023-08-05T15:15:09.064563",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.031186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       1\n",
      "2       0\n",
      "3       0\n",
      "4       1\n",
      "       ..\n",
      "8688    0\n",
      "8689    0\n",
      "8690    1\n",
      "8691    0\n",
      "8692    1\n",
      "Name: Transported, Length: 8693, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80ef7f",
   "metadata": {
    "id": "0-xUiOTwmnjY",
    "papermill": {
     "duration": 0.017935,
     "end_time": "2023-08-05T15:15:09.100769",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.082834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Select a Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ba6f1",
   "metadata": {
    "id": "sy81fpfxmnjY",
    "papermill": {
     "duration": 0.017629,
     "end_time": "2023-08-05T15:15:09.137413",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.119784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configure the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d62638f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:09.175976Z",
     "iopub.status.busy": "2023-08-05T15:15:09.175101Z",
     "iopub.status.idle": "2023-08-05T15:15:09.193383Z",
     "shell.execute_reply": "2023-08-05T15:15:09.192325Z"
    },
    "papermill": {
     "duration": 0.040824,
     "end_time": "2023-08-05T15:15:09.196169",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.155345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp_cat</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>dest_cat</th>\n",
       "      <th>VIP</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>deck_cat</th>\n",
       "      <th>side_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hp_cat  CryoSleep  dest_cat  VIP  FoodCourt  ShoppingMall     Spa  VRDeck  \\\n",
       "0       1          0         2    0        0.0           0.0     0.0     0.0   \n",
       "1       0          0         2    0        9.0          25.0   549.0    44.0   \n",
       "2       1          0         2    1     3576.0           0.0  6715.0    49.0   \n",
       "3       1          0         2    0     1283.0         371.0  3329.0   193.0   \n",
       "4       0          0         2    0       70.0         151.0   565.0     2.0   \n",
       "\n",
       "   deck_cat  side_cat  \n",
       "0         1         0  \n",
       "1         5         1  \n",
       "2         0         1  \n",
       "3         0         1  \n",
       "4         5         1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd33eb",
   "metadata": {
    "id": "wEQXtv4MmnjZ",
    "papermill": {
     "duration": 0.018103,
     "end_time": "2023-08-05T15:15:09.232873",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.214770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the model\n",
    "\n",
    "We will train the model using a one-liner.\n",
    "\n",
    "Note: you may see a warning about Autograph. You can safely ignore this, it will be fixed in the next release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dddab95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:09.272927Z",
     "iopub.status.busy": "2023-08-05T15:15:09.271984Z",
     "iopub.status.idle": "2023-08-05T15:15:09.278150Z",
     "shell.execute_reply": "2023-08-05T15:15:09.276782Z"
    },
    "papermill": {
     "duration": 0.029843,
     "end_time": "2023-08-05T15:15:09.281118",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.251275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# cat = CatBoostClassifier(iterations=1000,\n",
    "#                            learning_rate=0.5,\n",
    "#                            depth=2)\n",
    "\n",
    "# # Fit model\n",
    "\n",
    "# cat.fit(X_train, y_train)\n",
    "# # Get predicted classes\n",
    "# preds_class = cat.predict(X_test)\n",
    "# x_train_pred = cat.predict(X_train)\n",
    "# training_data_accuracy = accuracy_score(x_train_pred, y_train)\n",
    "# x_test_pred = cat.predict(X_test)\n",
    "# testing_data_accuracy = accuracy_score(x_test_pred, y_test)\n",
    "\n",
    "# # print(model.get_best_iteration())\n",
    "# print(\"Accuracy of training data: \",training_data_accuracy * 100)\n",
    "# print(\"Accuracy of testing data: \",testing_data_accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca1a16fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:09.321260Z",
     "iopub.status.busy": "2023-08-05T15:15:09.320754Z",
     "iopub.status.idle": "2023-08-05T15:15:09.610629Z",
     "shell.execute_reply": "2023-08-05T15:15:09.609271Z"
    },
    "papermill": {
     "duration": 0.321946,
     "end_time": "2023-08-05T15:15:09.622059",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.300113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,273\n",
      "Trainable params: 166,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa8ed6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:09.664771Z",
     "iopub.status.busy": "2023-08-05T15:15:09.664325Z",
     "iopub.status.idle": "2023-08-05T15:15:09.670779Z",
     "shell.execute_reply": "2023-08-05T15:15:09.669533Z"
    },
    "papermill": {
     "duration": 0.031266,
     "end_time": "2023-08-05T15:15:09.673689",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.642423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_name = 'best.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00f1c405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:15:09.716285Z",
     "iopub.status.busy": "2023-08-05T15:15:09.715830Z",
     "iopub.status.idle": "2023-08-05T15:21:28.074523Z",
     "shell.execute_reply": "2023-08-05T15:21:28.073361Z"
    },
    "papermill": {
     "duration": 378.383046,
     "end_time": "2023-08-05T15:21:28.077068",
     "exception": false,
     "start_time": "2023-08-05T15:15:09.694022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.8999 - mean_absolute_error: 0.8999\n",
      "Epoch 1: val_loss improved from inf to 0.32997, saving model to best.hdf5\n",
      "153/153 [==============================] - 2s 7ms/step - loss: 0.8999 - mean_absolute_error: 0.8999 - val_loss: 0.3300 - val_mean_absolute_error: 0.3300\n",
      "Epoch 2/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.3340 - mean_absolute_error: 0.3340\n",
      "Epoch 2: val_loss improved from 0.32997 to 0.28662, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.3336 - mean_absolute_error: 0.3336 - val_loss: 0.2866 - val_mean_absolute_error: 0.2866\n",
      "Epoch 3/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.3003 - mean_absolute_error: 0.3003\n",
      "Epoch 3: val_loss did not improve from 0.28662\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.3051 - mean_absolute_error: 0.3051 - val_loss: 0.3010 - val_mean_absolute_error: 0.3010\n",
      "Epoch 4/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2912 - mean_absolute_error: 0.2912\n",
      "Epoch 4: val_loss improved from 0.28662 to 0.26652, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2896 - mean_absolute_error: 0.2896 - val_loss: 0.2665 - val_mean_absolute_error: 0.2665\n",
      "Epoch 5/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2745 - mean_absolute_error: 0.2745\n",
      "Epoch 5: val_loss improved from 0.26652 to 0.26404, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2752 - mean_absolute_error: 0.2752 - val_loss: 0.2640 - val_mean_absolute_error: 0.2640\n",
      "Epoch 6/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2740 - mean_absolute_error: 0.2740\n",
      "Epoch 6: val_loss improved from 0.26404 to 0.26142, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2729 - mean_absolute_error: 0.2729 - val_loss: 0.2614 - val_mean_absolute_error: 0.2614\n",
      "Epoch 7/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2743 - mean_absolute_error: 0.2743\n",
      "Epoch 7: val_loss did not improve from 0.26142\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2736 - mean_absolute_error: 0.2736 - val_loss: 0.2660 - val_mean_absolute_error: 0.2660\n",
      "Epoch 8/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2663 - mean_absolute_error: 0.2663\n",
      "Epoch 8: val_loss improved from 0.26142 to 0.25602, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2655 - mean_absolute_error: 0.2655 - val_loss: 0.2560 - val_mean_absolute_error: 0.2560\n",
      "Epoch 9/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2638 - mean_absolute_error: 0.2638\n",
      "Epoch 9: val_loss improved from 0.25602 to 0.24456, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2629 - mean_absolute_error: 0.2629 - val_loss: 0.2446 - val_mean_absolute_error: 0.2446\n",
      "Epoch 10/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2623 - mean_absolute_error: 0.2623\n",
      "Epoch 10: val_loss did not improve from 0.24456\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2617 - mean_absolute_error: 0.2617 - val_loss: 0.2682 - val_mean_absolute_error: 0.2682\n",
      "Epoch 11/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2662 - mean_absolute_error: 0.2662\n",
      "Epoch 11: val_loss did not improve from 0.24456\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2652 - mean_absolute_error: 0.2652 - val_loss: 0.2538 - val_mean_absolute_error: 0.2538\n",
      "Epoch 12/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2616 - mean_absolute_error: 0.2616\n",
      "Epoch 12: val_loss improved from 0.24456 to 0.24331, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2603 - mean_absolute_error: 0.2603 - val_loss: 0.2433 - val_mean_absolute_error: 0.2433\n",
      "Epoch 13/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2631 - mean_absolute_error: 0.2631\n",
      "Epoch 13: val_loss did not improve from 0.24331\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2616 - mean_absolute_error: 0.2616 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "Epoch 14/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2540 - mean_absolute_error: 0.2540\n",
      "Epoch 14: val_loss did not improve from 0.24331\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2557 - mean_absolute_error: 0.2557 - val_loss: 0.2483 - val_mean_absolute_error: 0.2483\n",
      "Epoch 15/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2555 - mean_absolute_error: 0.2555\n",
      "Epoch 15: val_loss improved from 0.24331 to 0.24191, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2566 - mean_absolute_error: 0.2566 - val_loss: 0.2419 - val_mean_absolute_error: 0.2419\n",
      "Epoch 16/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2558 - mean_absolute_error: 0.2558\n",
      "Epoch 16: val_loss did not improve from 0.24191\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2562 - mean_absolute_error: 0.2562 - val_loss: 0.2434 - val_mean_absolute_error: 0.2434\n",
      "Epoch 17/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2501 - mean_absolute_error: 0.2501\n",
      "Epoch 17: val_loss improved from 0.24191 to 0.24124, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2492 - mean_absolute_error: 0.2492 - val_loss: 0.2412 - val_mean_absolute_error: 0.2412\n",
      "Epoch 18/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2480 - mean_absolute_error: 0.2480\n",
      "Epoch 18: val_loss improved from 0.24124 to 0.23670, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2475 - mean_absolute_error: 0.2475 - val_loss: 0.2367 - val_mean_absolute_error: 0.2367\n",
      "Epoch 19/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2455 - mean_absolute_error: 0.2455\n",
      "Epoch 19: val_loss improved from 0.23670 to 0.22681, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2452 - mean_absolute_error: 0.2452 - val_loss: 0.2268 - val_mean_absolute_error: 0.2268\n",
      "Epoch 20/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2416 - mean_absolute_error: 0.2416\n",
      "Epoch 20: val_loss did not improve from 0.22681\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2417 - mean_absolute_error: 0.2417 - val_loss: 0.2321 - val_mean_absolute_error: 0.2321\n",
      "Epoch 21/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2377 - mean_absolute_error: 0.2377\n",
      "Epoch 21: val_loss did not improve from 0.22681\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2377 - mean_absolute_error: 0.2377 - val_loss: 0.2476 - val_mean_absolute_error: 0.2476\n",
      "Epoch 22/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2360 - mean_absolute_error: 0.2360\n",
      "Epoch 22: val_loss improved from 0.22681 to 0.22251, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2351 - mean_absolute_error: 0.2351 - val_loss: 0.2225 - val_mean_absolute_error: 0.2225\n",
      "Epoch 23/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2338 - mean_absolute_error: 0.2338\n",
      "Epoch 23: val_loss did not improve from 0.22251\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2339 - mean_absolute_error: 0.2339 - val_loss: 0.2278 - val_mean_absolute_error: 0.2278\n",
      "Epoch 24/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2341 - mean_absolute_error: 0.2341\n",
      "Epoch 24: val_loss did not improve from 0.22251\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2341 - mean_absolute_error: 0.2341 - val_loss: 0.2262 - val_mean_absolute_error: 0.2262\n",
      "Epoch 25/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2333 - mean_absolute_error: 0.2333\n",
      "Epoch 25: val_loss improved from 0.22251 to 0.21890, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2328 - mean_absolute_error: 0.2328 - val_loss: 0.2189 - val_mean_absolute_error: 0.2189\n",
      "Epoch 26/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2360 - mean_absolute_error: 0.2360\n",
      "Epoch 26: val_loss did not improve from 0.21890\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2353 - mean_absolute_error: 0.2353 - val_loss: 0.2264 - val_mean_absolute_error: 0.2264\n",
      "Epoch 27/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2327 - mean_absolute_error: 0.2327\n",
      "Epoch 27: val_loss did not improve from 0.21890\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2347 - val_mean_absolute_error: 0.2347\n",
      "Epoch 28/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2354 - mean_absolute_error: 0.2354\n",
      "Epoch 28: val_loss did not improve from 0.21890\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2353 - mean_absolute_error: 0.2353 - val_loss: 0.2261 - val_mean_absolute_error: 0.2261\n",
      "Epoch 29/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2337 - mean_absolute_error: 0.2337\n",
      "Epoch 29: val_loss did not improve from 0.21890\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2292 - val_mean_absolute_error: 0.2292\n",
      "Epoch 30/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2335 - mean_absolute_error: 0.2335\n",
      "Epoch 30: val_loss did not improve from 0.21890\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2354 - mean_absolute_error: 0.2354 - val_loss: 0.2375 - val_mean_absolute_error: 0.2375\n",
      "Epoch 31/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2320 - mean_absolute_error: 0.2320\n",
      "Epoch 31: val_loss did not improve from 0.21890\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2243 - val_mean_absolute_error: 0.2243\n",
      "Epoch 32/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2332 - mean_absolute_error: 0.2332\n",
      "Epoch 32: val_loss improved from 0.21890 to 0.21374, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2137 - val_mean_absolute_error: 0.2137\n",
      "Epoch 33/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2345 - mean_absolute_error: 0.2345\n",
      "Epoch 33: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2337 - mean_absolute_error: 0.2337 - val_loss: 0.2195 - val_mean_absolute_error: 0.2195\n",
      "Epoch 34/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2297 - mean_absolute_error: 0.2297\n",
      "Epoch 34: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2155 - val_mean_absolute_error: 0.2155\n",
      "Epoch 35/500\n",
      "140/153 [==========================>...] - ETA: 0s - loss: 0.2304 - mean_absolute_error: 0.2304\n",
      "Epoch 35: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2330 - val_mean_absolute_error: 0.2330\n",
      "Epoch 36/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2344 - mean_absolute_error: 0.2344\n",
      "Epoch 36: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2348 - mean_absolute_error: 0.2348 - val_loss: 0.2274 - val_mean_absolute_error: 0.2274\n",
      "Epoch 37/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2338 - mean_absolute_error: 0.2338\n",
      "Epoch 37: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2169 - val_mean_absolute_error: 0.2169\n",
      "Epoch 38/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2282 - mean_absolute_error: 0.2282\n",
      "Epoch 38: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2276 - mean_absolute_error: 0.2276 - val_loss: 0.2287 - val_mean_absolute_error: 0.2287\n",
      "Epoch 39/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2280 - mean_absolute_error: 0.2280\n",
      "Epoch 39: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2277 - mean_absolute_error: 0.2277 - val_loss: 0.2241 - val_mean_absolute_error: 0.2241\n",
      "Epoch 40/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2274 - mean_absolute_error: 0.2274\n",
      "Epoch 40: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2304 - val_mean_absolute_error: 0.2304\n",
      "Epoch 41/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2346 - mean_absolute_error: 0.2346\n",
      "Epoch 41: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2234 - val_mean_absolute_error: 0.2234\n",
      "Epoch 42/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2302 - mean_absolute_error: 0.2302\n",
      "Epoch 42: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2230 - val_mean_absolute_error: 0.2230\n",
      "Epoch 43/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2280 - mean_absolute_error: 0.2280\n",
      "Epoch 43: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2278 - mean_absolute_error: 0.2278 - val_loss: 0.2286 - val_mean_absolute_error: 0.2286\n",
      "Epoch 44/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2267 - mean_absolute_error: 0.2267\n",
      "Epoch 44: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2261 - mean_absolute_error: 0.2261 - val_loss: 0.2192 - val_mean_absolute_error: 0.2192\n",
      "Epoch 45/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2274 - mean_absolute_error: 0.2274\n",
      "Epoch 45: val_loss did not improve from 0.21374\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2277 - mean_absolute_error: 0.2277 - val_loss: 0.2310 - val_mean_absolute_error: 0.2310\n",
      "Epoch 46/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2265 - mean_absolute_error: 0.2265\n",
      "Epoch 46: val_loss improved from 0.21374 to 0.21283, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2261 - mean_absolute_error: 0.2261 - val_loss: 0.2128 - val_mean_absolute_error: 0.2128\n",
      "Epoch 47/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2252 - mean_absolute_error: 0.2252\n",
      "Epoch 47: val_loss did not improve from 0.21283\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2259 - mean_absolute_error: 0.2259 - val_loss: 0.2199 - val_mean_absolute_error: 0.2199\n",
      "Epoch 48/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2237 - mean_absolute_error: 0.2237\n",
      "Epoch 48: val_loss improved from 0.21283 to 0.20964, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2248 - mean_absolute_error: 0.2248 - val_loss: 0.2096 - val_mean_absolute_error: 0.2096\n",
      "Epoch 49/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2267 - mean_absolute_error: 0.2267\n",
      "Epoch 49: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2272 - mean_absolute_error: 0.2272 - val_loss: 0.2332 - val_mean_absolute_error: 0.2332\n",
      "Epoch 50/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2282 - mean_absolute_error: 0.2282\n",
      "Epoch 50: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2292 - mean_absolute_error: 0.2292 - val_loss: 0.2268 - val_mean_absolute_error: 0.2268\n",
      "Epoch 51/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2249 - mean_absolute_error: 0.2249\n",
      "Epoch 51: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2241 - mean_absolute_error: 0.2241 - val_loss: 0.2107 - val_mean_absolute_error: 0.2107\n",
      "Epoch 52/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2218 - mean_absolute_error: 0.2218\n",
      "Epoch 52: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2218 - mean_absolute_error: 0.2218 - val_loss: 0.2195 - val_mean_absolute_error: 0.2195\n",
      "Epoch 53/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2205 - mean_absolute_error: 0.2205\n",
      "Epoch 53: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2211 - mean_absolute_error: 0.2211 - val_loss: 0.2119 - val_mean_absolute_error: 0.2119\n",
      "Epoch 54/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2227 - mean_absolute_error: 0.2227\n",
      "Epoch 54: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2208 - mean_absolute_error: 0.2208 - val_loss: 0.2146 - val_mean_absolute_error: 0.2146\n",
      "Epoch 55/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2205 - mean_absolute_error: 0.2205\n",
      "Epoch 55: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2216 - mean_absolute_error: 0.2216 - val_loss: 0.2152 - val_mean_absolute_error: 0.2152\n",
      "Epoch 56/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2181 - mean_absolute_error: 0.2181\n",
      "Epoch 56: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2191 - mean_absolute_error: 0.2191 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 57/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2207 - mean_absolute_error: 0.2207\n",
      "Epoch 57: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2222 - mean_absolute_error: 0.2222 - val_loss: 0.2328 - val_mean_absolute_error: 0.2328\n",
      "Epoch 58/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2243 - mean_absolute_error: 0.2243\n",
      "Epoch 58: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2232 - mean_absolute_error: 0.2232 - val_loss: 0.2205 - val_mean_absolute_error: 0.2205\n",
      "Epoch 59/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2221 - mean_absolute_error: 0.2221\n",
      "Epoch 59: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2210 - mean_absolute_error: 0.2210 - val_loss: 0.2152 - val_mean_absolute_error: 0.2152\n",
      "Epoch 60/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2249 - mean_absolute_error: 0.2249\n",
      "Epoch 60: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2244 - mean_absolute_error: 0.2244 - val_loss: 0.2186 - val_mean_absolute_error: 0.2186\n",
      "Epoch 61/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2187 - mean_absolute_error: 0.2187\n",
      "Epoch 61: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2191 - mean_absolute_error: 0.2191 - val_loss: 0.2107 - val_mean_absolute_error: 0.2107\n",
      "Epoch 62/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2308 - mean_absolute_error: 0.2308\n",
      "Epoch 62: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2300 - mean_absolute_error: 0.2300 - val_loss: 0.2239 - val_mean_absolute_error: 0.2239\n",
      "Epoch 63/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2198 - mean_absolute_error: 0.2198\n",
      "Epoch 63: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 64/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2165 - mean_absolute_error: 0.2165\n",
      "Epoch 64: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2177 - mean_absolute_error: 0.2177 - val_loss: 0.2122 - val_mean_absolute_error: 0.2122\n",
      "Epoch 65/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2164 - mean_absolute_error: 0.2164\n",
      "Epoch 65: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2191 - val_mean_absolute_error: 0.2191\n",
      "Epoch 66/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2158 - mean_absolute_error: 0.2158\n",
      "Epoch 66: val_loss did not improve from 0.20964\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2107 - val_mean_absolute_error: 0.2107\n",
      "Epoch 67/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2174 - mean_absolute_error: 0.2174\n",
      "Epoch 67: val_loss improved from 0.20964 to 0.20810, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2179 - mean_absolute_error: 0.2179 - val_loss: 0.2081 - val_mean_absolute_error: 0.2081\n",
      "Epoch 68/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2194 - mean_absolute_error: 0.2194\n",
      "Epoch 68: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2199 - mean_absolute_error: 0.2199 - val_loss: 0.2203 - val_mean_absolute_error: 0.2203\n",
      "Epoch 69/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2229 - mean_absolute_error: 0.2229\n",
      "Epoch 69: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2216 - mean_absolute_error: 0.2216 - val_loss: 0.2217 - val_mean_absolute_error: 0.2217\n",
      "Epoch 70/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2221 - mean_absolute_error: 0.2221\n",
      "Epoch 70: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2216 - mean_absolute_error: 0.2216 - val_loss: 0.2178 - val_mean_absolute_error: 0.2178\n",
      "Epoch 71/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2186 - mean_absolute_error: 0.2186\n",
      "Epoch 71: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2186 - mean_absolute_error: 0.2186 - val_loss: 0.2200 - val_mean_absolute_error: 0.2200\n",
      "Epoch 72/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2173 - mean_absolute_error: 0.2173\n",
      "Epoch 72: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2146 - val_mean_absolute_error: 0.2146\n",
      "Epoch 73/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2211 - mean_absolute_error: 0.2211\n",
      "Epoch 73: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2215 - mean_absolute_error: 0.2215 - val_loss: 0.2217 - val_mean_absolute_error: 0.2217\n",
      "Epoch 74/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2167 - mean_absolute_error: 0.2167\n",
      "Epoch 74: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2143 - val_mean_absolute_error: 0.2143\n",
      "Epoch 75/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2191 - mean_absolute_error: 0.2191\n",
      "Epoch 75: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2174 - mean_absolute_error: 0.2174 - val_loss: 0.2139 - val_mean_absolute_error: 0.2139\n",
      "Epoch 76/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2144 - mean_absolute_error: 0.2144\n",
      "Epoch 76: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2140 - mean_absolute_error: 0.2140 - val_loss: 0.2120 - val_mean_absolute_error: 0.2120\n",
      "Epoch 77/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2138 - mean_absolute_error: 0.2138\n",
      "Epoch 77: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2138 - mean_absolute_error: 0.2138 - val_loss: 0.2135 - val_mean_absolute_error: 0.2135\n",
      "Epoch 78/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2158 - mean_absolute_error: 0.2158\n",
      "Epoch 78: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2135 - mean_absolute_error: 0.2135 - val_loss: 0.2177 - val_mean_absolute_error: 0.2177\n",
      "Epoch 79/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2212 - mean_absolute_error: 0.2212\n",
      "Epoch 79: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2296 - val_mean_absolute_error: 0.2296\n",
      "Epoch 80/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2155 - mean_absolute_error: 0.2155\n",
      "Epoch 80: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2161 - mean_absolute_error: 0.2161 - val_loss: 0.2147 - val_mean_absolute_error: 0.2147\n",
      "Epoch 81/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2215 - mean_absolute_error: 0.2215\n",
      "Epoch 81: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2206 - mean_absolute_error: 0.2206 - val_loss: 0.2141 - val_mean_absolute_error: 0.2141\n",
      "Epoch 82/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2168 - mean_absolute_error: 0.2168\n",
      "Epoch 82: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2252 - val_mean_absolute_error: 0.2252\n",
      "Epoch 83/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2186 - mean_absolute_error: 0.2186\n",
      "Epoch 83: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2193 - mean_absolute_error: 0.2193 - val_loss: 0.2137 - val_mean_absolute_error: 0.2137\n",
      "Epoch 84/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2150 - mean_absolute_error: 0.2150\n",
      "Epoch 84: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2140 - mean_absolute_error: 0.2140 - val_loss: 0.2106 - val_mean_absolute_error: 0.2106\n",
      "Epoch 85/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2153 - mean_absolute_error: 0.2153\n",
      "Epoch 85: val_loss did not improve from 0.20810\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2132 - mean_absolute_error: 0.2132 - val_loss: 0.2113 - val_mean_absolute_error: 0.2113\n",
      "Epoch 86/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2205 - mean_absolute_error: 0.2205\n",
      "Epoch 86: val_loss improved from 0.20810 to 0.20703, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2205 - mean_absolute_error: 0.2205 - val_loss: 0.2070 - val_mean_absolute_error: 0.2070\n",
      "Epoch 87/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2147 - mean_absolute_error: 0.2147\n",
      "Epoch 87: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2140 - mean_absolute_error: 0.2140 - val_loss: 0.2147 - val_mean_absolute_error: 0.2147\n",
      "Epoch 88/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2139 - mean_absolute_error: 0.2139\n",
      "Epoch 88: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2125 - mean_absolute_error: 0.2125 - val_loss: 0.2192 - val_mean_absolute_error: 0.2192\n",
      "Epoch 89/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2151 - mean_absolute_error: 0.2151\n",
      "Epoch 89: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2145 - mean_absolute_error: 0.2145 - val_loss: 0.2244 - val_mean_absolute_error: 0.2244\n",
      "Epoch 90/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2235 - mean_absolute_error: 0.2235\n",
      "Epoch 90: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2203 - mean_absolute_error: 0.2203 - val_loss: 0.2092 - val_mean_absolute_error: 0.2092\n",
      "Epoch 91/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2134 - mean_absolute_error: 0.2134\n",
      "Epoch 91: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2130 - mean_absolute_error: 0.2130 - val_loss: 0.2134 - val_mean_absolute_error: 0.2134\n",
      "Epoch 92/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2117 - mean_absolute_error: 0.2117\n",
      "Epoch 92: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2102 - mean_absolute_error: 0.2102 - val_loss: 0.2180 - val_mean_absolute_error: 0.2180\n",
      "Epoch 93/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2179 - mean_absolute_error: 0.2179\n",
      "Epoch 93: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2112 - val_mean_absolute_error: 0.2112\n",
      "Epoch 94/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2156 - mean_absolute_error: 0.2156\n",
      "Epoch 94: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2143 - mean_absolute_error: 0.2143 - val_loss: 0.2139 - val_mean_absolute_error: 0.2139\n",
      "Epoch 95/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2091 - mean_absolute_error: 0.2091\n",
      "Epoch 95: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2107 - mean_absolute_error: 0.2107 - val_loss: 0.2147 - val_mean_absolute_error: 0.2147\n",
      "Epoch 96/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2125 - mean_absolute_error: 0.2125\n",
      "Epoch 96: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2120 - mean_absolute_error: 0.2120 - val_loss: 0.2073 - val_mean_absolute_error: 0.2073\n",
      "Epoch 97/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2089 - mean_absolute_error: 0.2089\n",
      "Epoch 97: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2101 - mean_absolute_error: 0.2101 - val_loss: 0.2254 - val_mean_absolute_error: 0.2254\n",
      "Epoch 98/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2113 - mean_absolute_error: 0.2113\n",
      "Epoch 98: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2111 - mean_absolute_error: 0.2111 - val_loss: 0.2190 - val_mean_absolute_error: 0.2190\n",
      "Epoch 99/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2109 - mean_absolute_error: 0.2109\n",
      "Epoch 99: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2102 - mean_absolute_error: 0.2102 - val_loss: 0.2129 - val_mean_absolute_error: 0.2129\n",
      "Epoch 100/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2106 - mean_absolute_error: 0.2106\n",
      "Epoch 100: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2109 - mean_absolute_error: 0.2109 - val_loss: 0.2281 - val_mean_absolute_error: 0.2281\n",
      "Epoch 101/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2101 - mean_absolute_error: 0.2101\n",
      "Epoch 101: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2137 - mean_absolute_error: 0.2137 - val_loss: 0.2187 - val_mean_absolute_error: 0.2187\n",
      "Epoch 102/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2106 - mean_absolute_error: 0.2106\n",
      "Epoch 102: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2106 - mean_absolute_error: 0.2106 - val_loss: 0.2244 - val_mean_absolute_error: 0.2244\n",
      "Epoch 103/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2217 - mean_absolute_error: 0.2217\n",
      "Epoch 103: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2221 - mean_absolute_error: 0.2221 - val_loss: 0.2152 - val_mean_absolute_error: 0.2152\n",
      "Epoch 104/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2104 - mean_absolute_error: 0.2104\n",
      "Epoch 104: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2089 - mean_absolute_error: 0.2089 - val_loss: 0.2101 - val_mean_absolute_error: 0.2101\n",
      "Epoch 105/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2109 - mean_absolute_error: 0.2109\n",
      "Epoch 105: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2096 - mean_absolute_error: 0.2096 - val_loss: 0.2183 - val_mean_absolute_error: 0.2183\n",
      "Epoch 106/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2116 - mean_absolute_error: 0.2116\n",
      "Epoch 106: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2102 - mean_absolute_error: 0.2102 - val_loss: 0.2117 - val_mean_absolute_error: 0.2117\n",
      "Epoch 107/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2088 - mean_absolute_error: 0.2088\n",
      "Epoch 107: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2078 - mean_absolute_error: 0.2078 - val_loss: 0.2080 - val_mean_absolute_error: 0.2080\n",
      "Epoch 108/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2083 - mean_absolute_error: 0.2083\n",
      "Epoch 108: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2093 - mean_absolute_error: 0.2093 - val_loss: 0.2132 - val_mean_absolute_error: 0.2132\n",
      "Epoch 109/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2084 - mean_absolute_error: 0.2084\n",
      "Epoch 109: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2096 - mean_absolute_error: 0.2096 - val_loss: 0.2087 - val_mean_absolute_error: 0.2087\n",
      "Epoch 110/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2107 - mean_absolute_error: 0.2107\n",
      "Epoch 110: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2110 - mean_absolute_error: 0.2110 - val_loss: 0.2224 - val_mean_absolute_error: 0.2224\n",
      "Epoch 111/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2161 - mean_absolute_error: 0.2161\n",
      "Epoch 111: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2229 - val_mean_absolute_error: 0.2229\n",
      "Epoch 112/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2140 - mean_absolute_error: 0.2140\n",
      "Epoch 112: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2153 - mean_absolute_error: 0.2153 - val_loss: 0.2139 - val_mean_absolute_error: 0.2139\n",
      "Epoch 113/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2141 - mean_absolute_error: 0.2141\n",
      "Epoch 113: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2139 - mean_absolute_error: 0.2139 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
      "Epoch 114/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2107 - mean_absolute_error: 0.2107\n",
      "Epoch 114: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2107 - mean_absolute_error: 0.2107 - val_loss: 0.2146 - val_mean_absolute_error: 0.2146\n",
      "Epoch 115/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2090 - mean_absolute_error: 0.2090\n",
      "Epoch 115: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2086 - mean_absolute_error: 0.2086 - val_loss: 0.2072 - val_mean_absolute_error: 0.2072\n",
      "Epoch 116/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 116: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2076 - mean_absolute_error: 0.2076 - val_loss: 0.2133 - val_mean_absolute_error: 0.2133\n",
      "Epoch 117/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 117: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2088 - mean_absolute_error: 0.2088 - val_loss: 0.2152 - val_mean_absolute_error: 0.2152\n",
      "Epoch 118/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2101 - mean_absolute_error: 0.2101\n",
      "Epoch 118: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2110 - mean_absolute_error: 0.2110 - val_loss: 0.2125 - val_mean_absolute_error: 0.2125\n",
      "Epoch 119/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2095 - mean_absolute_error: 0.2095\n",
      "Epoch 119: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2091 - mean_absolute_error: 0.2091 - val_loss: 0.2132 - val_mean_absolute_error: 0.2132\n",
      "Epoch 120/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2088 - mean_absolute_error: 0.2088\n",
      "Epoch 120: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2071 - mean_absolute_error: 0.2071 - val_loss: 0.2084 - val_mean_absolute_error: 0.2084\n",
      "Epoch 121/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2075 - mean_absolute_error: 0.2075\n",
      "Epoch 121: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2067 - mean_absolute_error: 0.2067 - val_loss: 0.2082 - val_mean_absolute_error: 0.2082\n",
      "Epoch 122/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 122: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2076 - mean_absolute_error: 0.2076 - val_loss: 0.2075 - val_mean_absolute_error: 0.2075\n",
      "Epoch 123/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2060 - mean_absolute_error: 0.2060\n",
      "Epoch 123: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2058 - mean_absolute_error: 0.2058 - val_loss: 0.2146 - val_mean_absolute_error: 0.2146\n",
      "Epoch 124/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2058 - mean_absolute_error: 0.2058\n",
      "Epoch 124: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2071 - mean_absolute_error: 0.2071 - val_loss: 0.2157 - val_mean_absolute_error: 0.2157\n",
      "Epoch 125/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2113 - mean_absolute_error: 0.2113\n",
      "Epoch 125: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2124 - mean_absolute_error: 0.2124 - val_loss: 0.2149 - val_mean_absolute_error: 0.2149\n",
      "Epoch 126/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2101 - mean_absolute_error: 0.2101\n",
      "Epoch 126: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2104 - mean_absolute_error: 0.2104 - val_loss: 0.2094 - val_mean_absolute_error: 0.2094\n",
      "Epoch 127/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2165 - mean_absolute_error: 0.2165\n",
      "Epoch 127: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2150 - mean_absolute_error: 0.2150 - val_loss: 0.2110 - val_mean_absolute_error: 0.2110\n",
      "Epoch 128/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2141 - mean_absolute_error: 0.2141\n",
      "Epoch 128: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2120 - mean_absolute_error: 0.2120 - val_loss: 0.2242 - val_mean_absolute_error: 0.2242\n",
      "Epoch 129/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2119 - mean_absolute_error: 0.2119\n",
      "Epoch 129: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2130 - mean_absolute_error: 0.2130 - val_loss: 0.2221 - val_mean_absolute_error: 0.2221\n",
      "Epoch 130/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 130: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2076 - mean_absolute_error: 0.2076 - val_loss: 0.2135 - val_mean_absolute_error: 0.2135\n",
      "Epoch 131/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2087 - mean_absolute_error: 0.2087\n",
      "Epoch 131: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2075 - mean_absolute_error: 0.2075 - val_loss: 0.2094 - val_mean_absolute_error: 0.2094\n",
      "Epoch 132/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2087 - mean_absolute_error: 0.2087\n",
      "Epoch 132: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2107 - mean_absolute_error: 0.2107 - val_loss: 0.2114 - val_mean_absolute_error: 0.2114\n",
      "Epoch 133/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2068 - mean_absolute_error: 0.2068\n",
      "Epoch 133: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2067 - mean_absolute_error: 0.2067 - val_loss: 0.2106 - val_mean_absolute_error: 0.2106\n",
      "Epoch 134/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2064 - mean_absolute_error: 0.2064\n",
      "Epoch 134: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2071 - mean_absolute_error: 0.2071 - val_loss: 0.2145 - val_mean_absolute_error: 0.2145\n",
      "Epoch 135/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2063 - mean_absolute_error: 0.2063\n",
      "Epoch 135: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2083 - mean_absolute_error: 0.2083 - val_loss: 0.2094 - val_mean_absolute_error: 0.2094\n",
      "Epoch 136/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2125 - mean_absolute_error: 0.2125\n",
      "Epoch 136: val_loss did not improve from 0.20703\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2128 - mean_absolute_error: 0.2128 - val_loss: 0.2202 - val_mean_absolute_error: 0.2202\n",
      "Epoch 137/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2096 - mean_absolute_error: 0.2096\n",
      "Epoch 137: val_loss improved from 0.20703 to 0.20682, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2096 - mean_absolute_error: 0.2096 - val_loss: 0.2068 - val_mean_absolute_error: 0.2068\n",
      "Epoch 138/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2063 - mean_absolute_error: 0.2063\n",
      "Epoch 138: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2088 - mean_absolute_error: 0.2088 - val_loss: 0.2109 - val_mean_absolute_error: 0.2109\n",
      "Epoch 139/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2091 - mean_absolute_error: 0.2091\n",
      "Epoch 139: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2082 - mean_absolute_error: 0.2082 - val_loss: 0.2188 - val_mean_absolute_error: 0.2188\n",
      "Epoch 140/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2105 - mean_absolute_error: 0.2105\n",
      "Epoch 140: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2098 - mean_absolute_error: 0.2098 - val_loss: 0.2151 - val_mean_absolute_error: 0.2151\n",
      "Epoch 141/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2048 - mean_absolute_error: 0.2048\n",
      "Epoch 141: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2059 - mean_absolute_error: 0.2059 - val_loss: 0.2267 - val_mean_absolute_error: 0.2267\n",
      "Epoch 142/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2072 - mean_absolute_error: 0.2072\n",
      "Epoch 142: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2066 - mean_absolute_error: 0.2066 - val_loss: 0.2117 - val_mean_absolute_error: 0.2117\n",
      "Epoch 143/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2054 - mean_absolute_error: 0.2054\n",
      "Epoch 143: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2053 - mean_absolute_error: 0.2053 - val_loss: 0.2127 - val_mean_absolute_error: 0.2127\n",
      "Epoch 144/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2059 - mean_absolute_error: 0.2059\n",
      "Epoch 144: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2062 - mean_absolute_error: 0.2062 - val_loss: 0.2141 - val_mean_absolute_error: 0.2141\n",
      "Epoch 145/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2093 - mean_absolute_error: 0.2093\n",
      "Epoch 145: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2093 - mean_absolute_error: 0.2093 - val_loss: 0.2094 - val_mean_absolute_error: 0.2094\n",
      "Epoch 146/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2122 - mean_absolute_error: 0.2122\n",
      "Epoch 146: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2134 - mean_absolute_error: 0.2134 - val_loss: 0.2192 - val_mean_absolute_error: 0.2192\n",
      "Epoch 147/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2182 - mean_absolute_error: 0.2182\n",
      "Epoch 147: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2112 - val_mean_absolute_error: 0.2112\n",
      "Epoch 148/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2160 - mean_absolute_error: 0.2160\n",
      "Epoch 148: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2160 - mean_absolute_error: 0.2160 - val_loss: 0.2143 - val_mean_absolute_error: 0.2143\n",
      "Epoch 149/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2129 - mean_absolute_error: 0.2129\n",
      "Epoch 149: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2129 - mean_absolute_error: 0.2129 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 150/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2166 - mean_absolute_error: 0.2166\n",
      "Epoch 150: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2190 - val_mean_absolute_error: 0.2190\n",
      "Epoch 151/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2175 - mean_absolute_error: 0.2175\n",
      "Epoch 151: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2161 - mean_absolute_error: 0.2161 - val_loss: 0.2112 - val_mean_absolute_error: 0.2112\n",
      "Epoch 152/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2079 - mean_absolute_error: 0.2079\n",
      "Epoch 152: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2079 - mean_absolute_error: 0.2079 - val_loss: 0.2147 - val_mean_absolute_error: 0.2147\n",
      "Epoch 153/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2057 - mean_absolute_error: 0.2057\n",
      "Epoch 153: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2094 - mean_absolute_error: 0.2094 - val_loss: 0.2201 - val_mean_absolute_error: 0.2201\n",
      "Epoch 154/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2101 - mean_absolute_error: 0.2101\n",
      "Epoch 154: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2117 - mean_absolute_error: 0.2117 - val_loss: 0.2105 - val_mean_absolute_error: 0.2105\n",
      "Epoch 155/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2117 - mean_absolute_error: 0.2117\n",
      "Epoch 155: val_loss did not improve from 0.20682\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2117 - mean_absolute_error: 0.2117 - val_loss: 0.2187 - val_mean_absolute_error: 0.2187\n",
      "Epoch 156/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2140 - mean_absolute_error: 0.2140\n",
      "Epoch 156: val_loss improved from 0.20682 to 0.20327, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2154 - mean_absolute_error: 0.2154 - val_loss: 0.2033 - val_mean_absolute_error: 0.2033\n",
      "Epoch 157/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2112 - mean_absolute_error: 0.2112\n",
      "Epoch 157: val_loss did not improve from 0.20327\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2111 - mean_absolute_error: 0.2111 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 158/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2165 - mean_absolute_error: 0.2165\n",
      "Epoch 158: val_loss did not improve from 0.20327\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2151 - mean_absolute_error: 0.2151 - val_loss: 0.2086 - val_mean_absolute_error: 0.2086\n",
      "Epoch 159/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2119 - mean_absolute_error: 0.2119\n",
      "Epoch 159: val_loss improved from 0.20327 to 0.20318, saving model to best.hdf5\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2094 - mean_absolute_error: 0.2094 - val_loss: 0.2032 - val_mean_absolute_error: 0.2032\n",
      "Epoch 160/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2056 - mean_absolute_error: 0.2056\n",
      "Epoch 160: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2102 - val_mean_absolute_error: 0.2102\n",
      "Epoch 161/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2111 - mean_absolute_error: 0.2111\n",
      "Epoch 161: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2096 - mean_absolute_error: 0.2096 - val_loss: 0.2069 - val_mean_absolute_error: 0.2069\n",
      "Epoch 162/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2068 - mean_absolute_error: 0.2068\n",
      "Epoch 162: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2060 - mean_absolute_error: 0.2060 - val_loss: 0.2053 - val_mean_absolute_error: 0.2053\n",
      "Epoch 163/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2069 - mean_absolute_error: 0.2069\n",
      "Epoch 163: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2085 - val_mean_absolute_error: 0.2085\n",
      "Epoch 164/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2088 - mean_absolute_error: 0.2088\n",
      "Epoch 164: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2099 - mean_absolute_error: 0.2099 - val_loss: 0.2109 - val_mean_absolute_error: 0.2109\n",
      "Epoch 165/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2107 - mean_absolute_error: 0.2107\n",
      "Epoch 165: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2077 - mean_absolute_error: 0.2077 - val_loss: 0.2080 - val_mean_absolute_error: 0.2080\n",
      "Epoch 166/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.2081\n",
      "Epoch 166: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2090 - mean_absolute_error: 0.2090 - val_loss: 0.2294 - val_mean_absolute_error: 0.2294\n",
      "Epoch 167/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2199 - mean_absolute_error: 0.2199\n",
      "Epoch 167: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.2192 - val_mean_absolute_error: 0.2192\n",
      "Epoch 168/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2037 - mean_absolute_error: 0.2037\n",
      "Epoch 168: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.2101 - val_mean_absolute_error: 0.2101\n",
      "Epoch 169/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2074 - mean_absolute_error: 0.2074\n",
      "Epoch 169: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2074 - mean_absolute_error: 0.2074 - val_loss: 0.2056 - val_mean_absolute_error: 0.2056\n",
      "Epoch 170/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 170: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2068 - mean_absolute_error: 0.2068 - val_loss: 0.2073 - val_mean_absolute_error: 0.2073\n",
      "Epoch 171/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2068 - mean_absolute_error: 0.2068\n",
      "Epoch 171: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2074 - mean_absolute_error: 0.2074 - val_loss: 0.2142 - val_mean_absolute_error: 0.2142\n",
      "Epoch 172/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2136 - mean_absolute_error: 0.2136\n",
      "Epoch 172: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2121 - mean_absolute_error: 0.2121 - val_loss: 0.2136 - val_mean_absolute_error: 0.2136\n",
      "Epoch 173/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2077 - mean_absolute_error: 0.2077\n",
      "Epoch 173: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2082 - mean_absolute_error: 0.2082 - val_loss: 0.2071 - val_mean_absolute_error: 0.2071\n",
      "Epoch 174/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2068 - mean_absolute_error: 0.2068\n",
      "Epoch 174: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2054 - mean_absolute_error: 0.2054 - val_loss: 0.2108 - val_mean_absolute_error: 0.2108\n",
      "Epoch 175/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2147 - mean_absolute_error: 0.2147\n",
      "Epoch 175: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2139 - mean_absolute_error: 0.2139 - val_loss: 0.2115 - val_mean_absolute_error: 0.2115\n",
      "Epoch 176/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 176: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2088 - mean_absolute_error: 0.2088 - val_loss: 0.2148 - val_mean_absolute_error: 0.2148\n",
      "Epoch 177/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2056 - mean_absolute_error: 0.2056\n",
      "Epoch 177: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2062 - mean_absolute_error: 0.2062 - val_loss: 0.2085 - val_mean_absolute_error: 0.2085\n",
      "Epoch 178/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 178: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2081 - mean_absolute_error: 0.2081 - val_loss: 0.2059 - val_mean_absolute_error: 0.2059\n",
      "Epoch 179/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2098 - mean_absolute_error: 0.2098\n",
      "Epoch 179: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2074 - mean_absolute_error: 0.2074 - val_loss: 0.2099 - val_mean_absolute_error: 0.2099\n",
      "Epoch 180/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2070 - mean_absolute_error: 0.2070\n",
      "Epoch 180: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2088 - val_mean_absolute_error: 0.2088\n",
      "Epoch 181/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2114 - mean_absolute_error: 0.2114\n",
      "Epoch 181: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2124 - mean_absolute_error: 0.2124 - val_loss: 0.2291 - val_mean_absolute_error: 0.2291\n",
      "Epoch 182/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2161 - mean_absolute_error: 0.2161\n",
      "Epoch 182: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2164 - mean_absolute_error: 0.2164 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 183/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2061 - mean_absolute_error: 0.2061\n",
      "Epoch 183: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2066 - mean_absolute_error: 0.2066 - val_loss: 0.2176 - val_mean_absolute_error: 0.2176\n",
      "Epoch 184/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2102 - mean_absolute_error: 0.2102\n",
      "Epoch 184: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2102 - mean_absolute_error: 0.2102 - val_loss: 0.2225 - val_mean_absolute_error: 0.2225\n",
      "Epoch 185/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2195 - mean_absolute_error: 0.2195\n",
      "Epoch 185: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2191 - mean_absolute_error: 0.2191 - val_loss: 0.2208 - val_mean_absolute_error: 0.2208\n",
      "Epoch 186/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2105 - mean_absolute_error: 0.2105\n",
      "Epoch 186: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2094 - mean_absolute_error: 0.2094 - val_loss: 0.2127 - val_mean_absolute_error: 0.2127\n",
      "Epoch 187/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2090 - mean_absolute_error: 0.2090\n",
      "Epoch 187: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2104 - mean_absolute_error: 0.2104 - val_loss: 0.2177 - val_mean_absolute_error: 0.2177\n",
      "Epoch 188/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2178 - mean_absolute_error: 0.2178\n",
      "Epoch 188: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2158 - mean_absolute_error: 0.2158 - val_loss: 0.2139 - val_mean_absolute_error: 0.2139\n",
      "Epoch 189/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2073 - mean_absolute_error: 0.2073\n",
      "Epoch 189: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2078 - mean_absolute_error: 0.2078 - val_loss: 0.2069 - val_mean_absolute_error: 0.2069\n",
      "Epoch 190/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2095 - mean_absolute_error: 0.2095\n",
      "Epoch 190: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2096 - mean_absolute_error: 0.2096 - val_loss: 0.2163 - val_mean_absolute_error: 0.2163\n",
      "Epoch 191/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2089 - mean_absolute_error: 0.2089\n",
      "Epoch 191: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2103 - mean_absolute_error: 0.2103 - val_loss: 0.2195 - val_mean_absolute_error: 0.2195\n",
      "Epoch 192/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2083 - mean_absolute_error: 0.2083\n",
      "Epoch 192: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2096 - mean_absolute_error: 0.2096 - val_loss: 0.2038 - val_mean_absolute_error: 0.2038\n",
      "Epoch 193/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2049 - mean_absolute_error: 0.2049\n",
      "Epoch 193: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.2090 - val_mean_absolute_error: 0.2090\n",
      "Epoch 194/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2051 - mean_absolute_error: 0.2051\n",
      "Epoch 194: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2052 - mean_absolute_error: 0.2052 - val_loss: 0.2043 - val_mean_absolute_error: 0.2043\n",
      "Epoch 195/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2067 - mean_absolute_error: 0.2067\n",
      "Epoch 195: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2067 - mean_absolute_error: 0.2067 - val_loss: 0.2049 - val_mean_absolute_error: 0.2049\n",
      "Epoch 196/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2054 - mean_absolute_error: 0.2054\n",
      "Epoch 196: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2054 - mean_absolute_error: 0.2054 - val_loss: 0.2074 - val_mean_absolute_error: 0.2074\n",
      "Epoch 197/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2049 - mean_absolute_error: 0.2049\n",
      "Epoch 197: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.2064 - val_mean_absolute_error: 0.2064\n",
      "Epoch 198/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2114 - mean_absolute_error: 0.2114\n",
      "Epoch 198: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2128 - mean_absolute_error: 0.2128 - val_loss: 0.2311 - val_mean_absolute_error: 0.2311\n",
      "Epoch 199/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2120 - mean_absolute_error: 0.2120\n",
      "Epoch 199: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2130 - mean_absolute_error: 0.2130 - val_loss: 0.2071 - val_mean_absolute_error: 0.2071\n",
      "Epoch 200/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2117 - mean_absolute_error: 0.2117\n",
      "Epoch 200: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2087 - mean_absolute_error: 0.2087 - val_loss: 0.2258 - val_mean_absolute_error: 0.2258\n",
      "Epoch 201/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2277 - mean_absolute_error: 0.2277\n",
      "Epoch 201: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2241 - mean_absolute_error: 0.2241 - val_loss: 0.2093 - val_mean_absolute_error: 0.2093\n",
      "Epoch 202/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2059 - mean_absolute_error: 0.2059\n",
      "Epoch 202: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2042 - mean_absolute_error: 0.2042 - val_loss: 0.2097 - val_mean_absolute_error: 0.2097\n",
      "Epoch 203/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2042 - mean_absolute_error: 0.2042\n",
      "Epoch 203: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2044 - mean_absolute_error: 0.2044 - val_loss: 0.2133 - val_mean_absolute_error: 0.2133\n",
      "Epoch 204/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2052 - mean_absolute_error: 0.2052\n",
      "Epoch 204: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.2131 - val_mean_absolute_error: 0.2131\n",
      "Epoch 205/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2042 - mean_absolute_error: 0.2042\n",
      "Epoch 205: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2040 - mean_absolute_error: 0.2040 - val_loss: 0.2127 - val_mean_absolute_error: 0.2127\n",
      "Epoch 206/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2069 - mean_absolute_error: 0.2069\n",
      "Epoch 206: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2077 - mean_absolute_error: 0.2077 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 207/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 207: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2090 - mean_absolute_error: 0.2090 - val_loss: 0.2141 - val_mean_absolute_error: 0.2141\n",
      "Epoch 208/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2077 - mean_absolute_error: 0.2077\n",
      "Epoch 208: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2077 - mean_absolute_error: 0.2077 - val_loss: 0.2045 - val_mean_absolute_error: 0.2045\n",
      "Epoch 209/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2065 - mean_absolute_error: 0.2065\n",
      "Epoch 209: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2083 - mean_absolute_error: 0.2083 - val_loss: 0.2081 - val_mean_absolute_error: 0.2081\n",
      "Epoch 210/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2063 - mean_absolute_error: 0.2063\n",
      "Epoch 210: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2071 - mean_absolute_error: 0.2071 - val_loss: 0.2085 - val_mean_absolute_error: 0.2085\n",
      "Epoch 211/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2059 - mean_absolute_error: 0.2059\n",
      "Epoch 211: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2051 - mean_absolute_error: 0.2051 - val_loss: 0.2097 - val_mean_absolute_error: 0.2097\n",
      "Epoch 212/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2027 - mean_absolute_error: 0.2027\n",
      "Epoch 212: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2027 - mean_absolute_error: 0.2027 - val_loss: 0.2070 - val_mean_absolute_error: 0.2070\n",
      "Epoch 213/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2008 - mean_absolute_error: 0.2008\n",
      "Epoch 213: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2028 - mean_absolute_error: 0.2028 - val_loss: 0.2086 - val_mean_absolute_error: 0.2086\n",
      "Epoch 214/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2036 - mean_absolute_error: 0.2036\n",
      "Epoch 214: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2036 - mean_absolute_error: 0.2036 - val_loss: 0.2097 - val_mean_absolute_error: 0.2097\n",
      "Epoch 215/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2039 - mean_absolute_error: 0.2039\n",
      "Epoch 215: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2036 - mean_absolute_error: 0.2036 - val_loss: 0.2103 - val_mean_absolute_error: 0.2103\n",
      "Epoch 216/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2073 - mean_absolute_error: 0.2073\n",
      "Epoch 216: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2059 - mean_absolute_error: 0.2059 - val_loss: 0.2062 - val_mean_absolute_error: 0.2062\n",
      "Epoch 217/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2044 - mean_absolute_error: 0.2044\n",
      "Epoch 217: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2044 - mean_absolute_error: 0.2044 - val_loss: 0.2161 - val_mean_absolute_error: 0.2161\n",
      "Epoch 218/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2066 - mean_absolute_error: 0.2066\n",
      "Epoch 218: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2061 - mean_absolute_error: 0.2061 - val_loss: 0.2175 - val_mean_absolute_error: 0.2175\n",
      "Epoch 219/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2075 - mean_absolute_error: 0.2075\n",
      "Epoch 219: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2097 - mean_absolute_error: 0.2097 - val_loss: 0.2076 - val_mean_absolute_error: 0.2076\n",
      "Epoch 220/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2062 - mean_absolute_error: 0.2062\n",
      "Epoch 220: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2064 - mean_absolute_error: 0.2064 - val_loss: 0.2116 - val_mean_absolute_error: 0.2116\n",
      "Epoch 221/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2088 - mean_absolute_error: 0.2088\n",
      "Epoch 221: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2103 - mean_absolute_error: 0.2103 - val_loss: 0.2138 - val_mean_absolute_error: 0.2138\n",
      "Epoch 222/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2024 - mean_absolute_error: 0.2024\n",
      "Epoch 222: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2022 - mean_absolute_error: 0.2022 - val_loss: 0.2128 - val_mean_absolute_error: 0.2128\n",
      "Epoch 223/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2063 - mean_absolute_error: 0.2063\n",
      "Epoch 223: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2054 - mean_absolute_error: 0.2054 - val_loss: 0.2171 - val_mean_absolute_error: 0.2171\n",
      "Epoch 224/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2064 - mean_absolute_error: 0.2064\n",
      "Epoch 224: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2053 - mean_absolute_error: 0.2053 - val_loss: 0.2079 - val_mean_absolute_error: 0.2079\n",
      "Epoch 225/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2035 - mean_absolute_error: 0.2035\n",
      "Epoch 225: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2037 - mean_absolute_error: 0.2037 - val_loss: 0.2145 - val_mean_absolute_error: 0.2145\n",
      "Epoch 226/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2033 - mean_absolute_error: 0.2033\n",
      "Epoch 226: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2032 - mean_absolute_error: 0.2032 - val_loss: 0.2095 - val_mean_absolute_error: 0.2095\n",
      "Epoch 227/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2055 - mean_absolute_error: 0.2055\n",
      "Epoch 227: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2037 - mean_absolute_error: 0.2037 - val_loss: 0.2099 - val_mean_absolute_error: 0.2099\n",
      "Epoch 228/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2092 - mean_absolute_error: 0.2092\n",
      "Epoch 228: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2078 - mean_absolute_error: 0.2078 - val_loss: 0.2142 - val_mean_absolute_error: 0.2142\n",
      "Epoch 229/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2128 - mean_absolute_error: 0.2128\n",
      "Epoch 229: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2133 - mean_absolute_error: 0.2133 - val_loss: 0.2149 - val_mean_absolute_error: 0.2149\n",
      "Epoch 230/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2172 - mean_absolute_error: 0.2172\n",
      "Epoch 230: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2133 - mean_absolute_error: 0.2133 - val_loss: 0.2096 - val_mean_absolute_error: 0.2096\n",
      "Epoch 231/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2048 - mean_absolute_error: 0.2048\n",
      "Epoch 231: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2068 - mean_absolute_error: 0.2068 - val_loss: 0.2121 - val_mean_absolute_error: 0.2121\n",
      "Epoch 232/500\n",
      "140/153 [==========================>...] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 232: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2066 - mean_absolute_error: 0.2066 - val_loss: 0.2125 - val_mean_absolute_error: 0.2125\n",
      "Epoch 233/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2057 - mean_absolute_error: 0.2057\n",
      "Epoch 233: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2062 - mean_absolute_error: 0.2062 - val_loss: 0.2049 - val_mean_absolute_error: 0.2049\n",
      "Epoch 234/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2024 - mean_absolute_error: 0.2024\n",
      "Epoch 234: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2033 - mean_absolute_error: 0.2033 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 235/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2068 - mean_absolute_error: 0.2068\n",
      "Epoch 235: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2073 - mean_absolute_error: 0.2073 - val_loss: 0.2237 - val_mean_absolute_error: 0.2237\n",
      "Epoch 236/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2067 - mean_absolute_error: 0.2067\n",
      "Epoch 236: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2075 - mean_absolute_error: 0.2075 - val_loss: 0.2179 - val_mean_absolute_error: 0.2179\n",
      "Epoch 237/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2061 - mean_absolute_error: 0.2061\n",
      "Epoch 237: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2071 - mean_absolute_error: 0.2071 - val_loss: 0.2329 - val_mean_absolute_error: 0.2329\n",
      "Epoch 238/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2292 - mean_absolute_error: 0.2292\n",
      "Epoch 238: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2292 - mean_absolute_error: 0.2292 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 239/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2130 - mean_absolute_error: 0.2130\n",
      "Epoch 239: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2148 - mean_absolute_error: 0.2148 - val_loss: 0.2165 - val_mean_absolute_error: 0.2165\n",
      "Epoch 240/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2129 - mean_absolute_error: 0.2129\n",
      "Epoch 240: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2130 - mean_absolute_error: 0.2130 - val_loss: 0.2125 - val_mean_absolute_error: 0.2125\n",
      "Epoch 241/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2087 - mean_absolute_error: 0.2087\n",
      "Epoch 241: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2094 - mean_absolute_error: 0.2094 - val_loss: 0.2049 - val_mean_absolute_error: 0.2049\n",
      "Epoch 242/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2084 - mean_absolute_error: 0.2084\n",
      "Epoch 242: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2079 - mean_absolute_error: 0.2079 - val_loss: 0.2078 - val_mean_absolute_error: 0.2078\n",
      "Epoch 243/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2106 - mean_absolute_error: 0.2106\n",
      "Epoch 243: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2081 - mean_absolute_error: 0.2081 - val_loss: 0.2095 - val_mean_absolute_error: 0.2095\n",
      "Epoch 244/500\n",
      "139/153 [==========================>...] - ETA: 0s - loss: 0.2045 - mean_absolute_error: 0.2045\n",
      "Epoch 244: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2059 - mean_absolute_error: 0.2059 - val_loss: 0.2060 - val_mean_absolute_error: 0.2060\n",
      "Epoch 245/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2020 - mean_absolute_error: 0.2020\n",
      "Epoch 245: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2024 - mean_absolute_error: 0.2024 - val_loss: 0.2081 - val_mean_absolute_error: 0.2081\n",
      "Epoch 246/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2041 - mean_absolute_error: 0.2041\n",
      "Epoch 246: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2031 - mean_absolute_error: 0.2031 - val_loss: 0.2134 - val_mean_absolute_error: 0.2134\n",
      "Epoch 247/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2052 - mean_absolute_error: 0.2052\n",
      "Epoch 247: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2054 - mean_absolute_error: 0.2054 - val_loss: 0.2048 - val_mean_absolute_error: 0.2048\n",
      "Epoch 248/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2029 - mean_absolute_error: 0.2029\n",
      "Epoch 248: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2038 - mean_absolute_error: 0.2038 - val_loss: 0.2156 - val_mean_absolute_error: 0.2156\n",
      "Epoch 249/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2047 - mean_absolute_error: 0.2047\n",
      "Epoch 249: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.2098 - val_mean_absolute_error: 0.2098\n",
      "Epoch 250/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2053 - mean_absolute_error: 0.2053\n",
      "Epoch 250: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2035 - mean_absolute_error: 0.2035 - val_loss: 0.2101 - val_mean_absolute_error: 0.2101\n",
      "Epoch 251/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2030 - mean_absolute_error: 0.2030\n",
      "Epoch 251: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2026 - mean_absolute_error: 0.2026 - val_loss: 0.2042 - val_mean_absolute_error: 0.2042\n",
      "Epoch 252/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2057 - mean_absolute_error: 0.2057\n",
      "Epoch 252: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2058 - mean_absolute_error: 0.2058 - val_loss: 0.2097 - val_mean_absolute_error: 0.2097\n",
      "Epoch 253/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2024 - mean_absolute_error: 0.2024\n",
      "Epoch 253: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2022 - mean_absolute_error: 0.2022 - val_loss: 0.2055 - val_mean_absolute_error: 0.2055\n",
      "Epoch 254/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2084 - mean_absolute_error: 0.2084\n",
      "Epoch 254: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2083 - mean_absolute_error: 0.2083 - val_loss: 0.2328 - val_mean_absolute_error: 0.2328\n",
      "Epoch 255/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2170 - mean_absolute_error: 0.2170\n",
      "Epoch 255: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2292 - val_mean_absolute_error: 0.2292\n",
      "Epoch 256/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2099 - mean_absolute_error: 0.2099\n",
      "Epoch 256: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2089 - mean_absolute_error: 0.2089 - val_loss: 0.2095 - val_mean_absolute_error: 0.2095\n",
      "Epoch 257/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2037 - mean_absolute_error: 0.2037\n",
      "Epoch 257: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2040 - mean_absolute_error: 0.2040 - val_loss: 0.2085 - val_mean_absolute_error: 0.2085\n",
      "Epoch 258/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2125 - mean_absolute_error: 0.2125\n",
      "Epoch 258: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2125 - mean_absolute_error: 0.2125 - val_loss: 0.2063 - val_mean_absolute_error: 0.2063\n",
      "Epoch 259/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 259: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2085 - mean_absolute_error: 0.2085 - val_loss: 0.2194 - val_mean_absolute_error: 0.2194\n",
      "Epoch 260/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2030 - mean_absolute_error: 0.2030\n",
      "Epoch 260: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2041 - mean_absolute_error: 0.2041 - val_loss: 0.2176 - val_mean_absolute_error: 0.2176\n",
      "Epoch 261/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2110 - mean_absolute_error: 0.2110\n",
      "Epoch 261: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2086 - mean_absolute_error: 0.2086 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
      "Epoch 262/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 262: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2084 - mean_absolute_error: 0.2084 - val_loss: 0.2108 - val_mean_absolute_error: 0.2108\n",
      "Epoch 263/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2078 - mean_absolute_error: 0.2078\n",
      "Epoch 263: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2077 - mean_absolute_error: 0.2077 - val_loss: 0.2261 - val_mean_absolute_error: 0.2261\n",
      "Epoch 264/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2220 - mean_absolute_error: 0.2220\n",
      "Epoch 264: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2222 - mean_absolute_error: 0.2222 - val_loss: 0.2196 - val_mean_absolute_error: 0.2196\n",
      "Epoch 265/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2172 - mean_absolute_error: 0.2172\n",
      "Epoch 265: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2218 - val_mean_absolute_error: 0.2218\n",
      "Epoch 266/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2128 - mean_absolute_error: 0.2128\n",
      "Epoch 266: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2149 - mean_absolute_error: 0.2149 - val_loss: 0.2165 - val_mean_absolute_error: 0.2165\n",
      "Epoch 267/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2176 - mean_absolute_error: 0.2176\n",
      "Epoch 267: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2177 - mean_absolute_error: 0.2177 - val_loss: 0.2203 - val_mean_absolute_error: 0.2203\n",
      "Epoch 268/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2177 - mean_absolute_error: 0.2177\n",
      "Epoch 268: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2161 - mean_absolute_error: 0.2161 - val_loss: 0.2205 - val_mean_absolute_error: 0.2205\n",
      "Epoch 269/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2153 - mean_absolute_error: 0.2153\n",
      "Epoch 269: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2240 - val_mean_absolute_error: 0.2240\n",
      "Epoch 270/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2186 - mean_absolute_error: 0.2186\n",
      "Epoch 270: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2210 - val_mean_absolute_error: 0.2210\n",
      "Epoch 271/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2179 - mean_absolute_error: 0.2179\n",
      "Epoch 271: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2206 - val_mean_absolute_error: 0.2206\n",
      "Epoch 272/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2140 - mean_absolute_error: 0.2140\n",
      "Epoch 272: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2147 - mean_absolute_error: 0.2147 - val_loss: 0.2229 - val_mean_absolute_error: 0.2229\n",
      "Epoch 273/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2119 - mean_absolute_error: 0.2119\n",
      "Epoch 273: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2119 - mean_absolute_error: 0.2119 - val_loss: 0.2324 - val_mean_absolute_error: 0.2324\n",
      "Epoch 274/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2094 - mean_absolute_error: 0.2094\n",
      "Epoch 274: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2078 - mean_absolute_error: 0.2078 - val_loss: 0.2206 - val_mean_absolute_error: 0.2206\n",
      "Epoch 275/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2060 - mean_absolute_error: 0.2060\n",
      "Epoch 275: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2062 - mean_absolute_error: 0.2062 - val_loss: 0.2222 - val_mean_absolute_error: 0.2222\n",
      "Epoch 276/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2054 - mean_absolute_error: 0.2054\n",
      "Epoch 276: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2050 - mean_absolute_error: 0.2050 - val_loss: 0.2153 - val_mean_absolute_error: 0.2153\n",
      "Epoch 277/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2142 - mean_absolute_error: 0.2142\n",
      "Epoch 277: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2132 - mean_absolute_error: 0.2132 - val_loss: 0.2245 - val_mean_absolute_error: 0.2245\n",
      "Epoch 278/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2062 - mean_absolute_error: 0.2062\n",
      "Epoch 278: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2087 - mean_absolute_error: 0.2087 - val_loss: 0.2225 - val_mean_absolute_error: 0.2225\n",
      "Epoch 279/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2063 - mean_absolute_error: 0.2063\n",
      "Epoch 279: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2074 - mean_absolute_error: 0.2074 - val_loss: 0.2169 - val_mean_absolute_error: 0.2169\n",
      "Epoch 280/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2069 - mean_absolute_error: 0.2069\n",
      "Epoch 280: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 281/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2053 - mean_absolute_error: 0.2053\n",
      "Epoch 281: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2065 - mean_absolute_error: 0.2065 - val_loss: 0.2189 - val_mean_absolute_error: 0.2189\n",
      "Epoch 282/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2057 - mean_absolute_error: 0.2057\n",
      "Epoch 282: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2060 - mean_absolute_error: 0.2060 - val_loss: 0.2144 - val_mean_absolute_error: 0.2144\n",
      "Epoch 283/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2042 - mean_absolute_error: 0.2042\n",
      "Epoch 283: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2039 - mean_absolute_error: 0.2039 - val_loss: 0.2225 - val_mean_absolute_error: 0.2225\n",
      "Epoch 284/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2047 - mean_absolute_error: 0.2047\n",
      "Epoch 284: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2049 - mean_absolute_error: 0.2049 - val_loss: 0.2226 - val_mean_absolute_error: 0.2226\n",
      "Epoch 285/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2115 - mean_absolute_error: 0.2115\n",
      "Epoch 285: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2106 - mean_absolute_error: 0.2106 - val_loss: 0.2195 - val_mean_absolute_error: 0.2195\n",
      "Epoch 286/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2068 - mean_absolute_error: 0.2068\n",
      "Epoch 286: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2048 - mean_absolute_error: 0.2048 - val_loss: 0.2164 - val_mean_absolute_error: 0.2164\n",
      "Epoch 287/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2040 - mean_absolute_error: 0.2040\n",
      "Epoch 287: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2056 - mean_absolute_error: 0.2056 - val_loss: 0.2175 - val_mean_absolute_error: 0.2175\n",
      "Epoch 288/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2038 - mean_absolute_error: 0.2038\n",
      "Epoch 288: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2048 - mean_absolute_error: 0.2048 - val_loss: 0.2117 - val_mean_absolute_error: 0.2117\n",
      "Epoch 289/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2064 - mean_absolute_error: 0.2064\n",
      "Epoch 289: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2057 - mean_absolute_error: 0.2057 - val_loss: 0.2185 - val_mean_absolute_error: 0.2185\n",
      "Epoch 290/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2048 - mean_absolute_error: 0.2048\n",
      "Epoch 290: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2048 - mean_absolute_error: 0.2048 - val_loss: 0.2240 - val_mean_absolute_error: 0.2240\n",
      "Epoch 291/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2082 - mean_absolute_error: 0.2082\n",
      "Epoch 291: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2083 - mean_absolute_error: 0.2083 - val_loss: 0.2218 - val_mean_absolute_error: 0.2218\n",
      "Epoch 292/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2067 - mean_absolute_error: 0.2067\n",
      "Epoch 292: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2073 - mean_absolute_error: 0.2073 - val_loss: 0.2207 - val_mean_absolute_error: 0.2207\n",
      "Epoch 293/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2079 - mean_absolute_error: 0.2079\n",
      "Epoch 293: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2082 - mean_absolute_error: 0.2082 - val_loss: 0.2194 - val_mean_absolute_error: 0.2194\n",
      "Epoch 294/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.2081\n",
      "Epoch 294: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2083 - mean_absolute_error: 0.2083 - val_loss: 0.2142 - val_mean_absolute_error: 0.2142\n",
      "Epoch 295/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2164 - mean_absolute_error: 0.2164\n",
      "Epoch 295: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2146 - mean_absolute_error: 0.2146 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 296/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2116 - mean_absolute_error: 0.2116\n",
      "Epoch 296: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2141 - mean_absolute_error: 0.2141 - val_loss: 0.2275 - val_mean_absolute_error: 0.2275\n",
      "Epoch 297/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.2081\n",
      "Epoch 297: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2088 - mean_absolute_error: 0.2088 - val_loss: 0.2160 - val_mean_absolute_error: 0.2160\n",
      "Epoch 298/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2056 - mean_absolute_error: 0.2056\n",
      "Epoch 298: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2055 - mean_absolute_error: 0.2055 - val_loss: 0.2143 - val_mean_absolute_error: 0.2143\n",
      "Epoch 299/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2065 - mean_absolute_error: 0.2065\n",
      "Epoch 299: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2080 - mean_absolute_error: 0.2080 - val_loss: 0.2151 - val_mean_absolute_error: 0.2151\n",
      "Epoch 300/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2110 - mean_absolute_error: 0.2110\n",
      "Epoch 300: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2104 - mean_absolute_error: 0.2104 - val_loss: 0.2141 - val_mean_absolute_error: 0.2141\n",
      "Epoch 301/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2133 - mean_absolute_error: 0.2133\n",
      "Epoch 301: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2112 - mean_absolute_error: 0.2112 - val_loss: 0.2237 - val_mean_absolute_error: 0.2237\n",
      "Epoch 302/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2096 - mean_absolute_error: 0.2096\n",
      "Epoch 302: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2091 - mean_absolute_error: 0.2091 - val_loss: 0.2189 - val_mean_absolute_error: 0.2189\n",
      "Epoch 303/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2059 - mean_absolute_error: 0.2059\n",
      "Epoch 303: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2060 - mean_absolute_error: 0.2060 - val_loss: 0.2269 - val_mean_absolute_error: 0.2269\n",
      "Epoch 304/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2098 - mean_absolute_error: 0.2098\n",
      "Epoch 304: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2089 - mean_absolute_error: 0.2089 - val_loss: 0.2061 - val_mean_absolute_error: 0.2061\n",
      "Epoch 305/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2085 - mean_absolute_error: 0.2085\n",
      "Epoch 305: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2094 - mean_absolute_error: 0.2094 - val_loss: 0.2140 - val_mean_absolute_error: 0.2140\n",
      "Epoch 306/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2049 - mean_absolute_error: 0.2049\n",
      "Epoch 306: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2063 - mean_absolute_error: 0.2063 - val_loss: 0.2115 - val_mean_absolute_error: 0.2115\n",
      "Epoch 307/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2083 - mean_absolute_error: 0.2083\n",
      "Epoch 307: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2081 - mean_absolute_error: 0.2081 - val_loss: 0.2141 - val_mean_absolute_error: 0.2141\n",
      "Epoch 308/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2096 - mean_absolute_error: 0.2096\n",
      "Epoch 308: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2095 - mean_absolute_error: 0.2095 - val_loss: 0.2187 - val_mean_absolute_error: 0.2187\n",
      "Epoch 309/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2207 - mean_absolute_error: 0.2207\n",
      "Epoch 309: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2205 - mean_absolute_error: 0.2205 - val_loss: 0.2265 - val_mean_absolute_error: 0.2265\n",
      "Epoch 310/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2139 - mean_absolute_error: 0.2139\n",
      "Epoch 310: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2140 - mean_absolute_error: 0.2140 - val_loss: 0.2137 - val_mean_absolute_error: 0.2137\n",
      "Epoch 311/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2236 - mean_absolute_error: 0.2236\n",
      "Epoch 311: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2215 - mean_absolute_error: 0.2215 - val_loss: 0.2101 - val_mean_absolute_error: 0.2101\n",
      "Epoch 312/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2093 - mean_absolute_error: 0.2093\n",
      "Epoch 312: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2098 - mean_absolute_error: 0.2098 - val_loss: 0.2160 - val_mean_absolute_error: 0.2160\n",
      "Epoch 313/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2082 - mean_absolute_error: 0.2082\n",
      "Epoch 313: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2082 - mean_absolute_error: 0.2082 - val_loss: 0.2123 - val_mean_absolute_error: 0.2123\n",
      "Epoch 314/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.2081\n",
      "Epoch 314: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2087 - mean_absolute_error: 0.2087 - val_loss: 0.2110 - val_mean_absolute_error: 0.2110\n",
      "Epoch 315/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2137 - mean_absolute_error: 0.2137\n",
      "Epoch 315: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2134 - mean_absolute_error: 0.2134 - val_loss: 0.2158 - val_mean_absolute_error: 0.2158\n",
      "Epoch 316/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2177 - mean_absolute_error: 0.2177\n",
      "Epoch 316: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2096 - val_mean_absolute_error: 0.2096\n",
      "Epoch 317/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2141 - mean_absolute_error: 0.2141\n",
      "Epoch 317: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2133 - mean_absolute_error: 0.2133 - val_loss: 0.2124 - val_mean_absolute_error: 0.2124\n",
      "Epoch 318/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2091 - mean_absolute_error: 0.2091\n",
      "Epoch 318: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2084 - mean_absolute_error: 0.2084 - val_loss: 0.2245 - val_mean_absolute_error: 0.2245\n",
      "Epoch 319/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2118 - mean_absolute_error: 0.2118\n",
      "Epoch 319: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2132 - mean_absolute_error: 0.2132 - val_loss: 0.2203 - val_mean_absolute_error: 0.2203\n",
      "Epoch 320/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2199 - mean_absolute_error: 0.2199\n",
      "Epoch 320: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2243 - val_mean_absolute_error: 0.2243\n",
      "Epoch 321/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2148 - mean_absolute_error: 0.2148\n",
      "Epoch 321: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2129 - mean_absolute_error: 0.2129 - val_loss: 0.2113 - val_mean_absolute_error: 0.2113\n",
      "Epoch 322/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2141 - mean_absolute_error: 0.2141\n",
      "Epoch 322: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2134 - mean_absolute_error: 0.2134 - val_loss: 0.2169 - val_mean_absolute_error: 0.2169\n",
      "Epoch 323/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2035 - mean_absolute_error: 0.2035\n",
      "Epoch 323: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2052 - mean_absolute_error: 0.2052 - val_loss: 0.2094 - val_mean_absolute_error: 0.2094\n",
      "Epoch 324/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2087 - mean_absolute_error: 0.2087\n",
      "Epoch 324: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2075 - mean_absolute_error: 0.2075 - val_loss: 0.2097 - val_mean_absolute_error: 0.2097\n",
      "Epoch 325/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2065 - mean_absolute_error: 0.2065\n",
      "Epoch 325: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2062 - mean_absolute_error: 0.2062 - val_loss: 0.2119 - val_mean_absolute_error: 0.2119\n",
      "Epoch 326/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2058 - mean_absolute_error: 0.2058\n",
      "Epoch 326: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2073 - mean_absolute_error: 0.2073 - val_loss: 0.2147 - val_mean_absolute_error: 0.2147\n",
      "Epoch 327/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2046 - mean_absolute_error: 0.2046\n",
      "Epoch 327: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2051 - mean_absolute_error: 0.2051 - val_loss: 0.2141 - val_mean_absolute_error: 0.2141\n",
      "Epoch 328/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2090 - mean_absolute_error: 0.2090\n",
      "Epoch 328: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2085 - mean_absolute_error: 0.2085 - val_loss: 0.2188 - val_mean_absolute_error: 0.2188\n",
      "Epoch 329/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2049 - mean_absolute_error: 0.2049\n",
      "Epoch 329: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2045 - mean_absolute_error: 0.2045 - val_loss: 0.2102 - val_mean_absolute_error: 0.2102\n",
      "Epoch 330/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2051 - mean_absolute_error: 0.2051\n",
      "Epoch 330: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2086 - mean_absolute_error: 0.2086 - val_loss: 0.2347 - val_mean_absolute_error: 0.2347\n",
      "Epoch 331/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2146 - mean_absolute_error: 0.2146\n",
      "Epoch 331: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2133 - mean_absolute_error: 0.2133 - val_loss: 0.2056 - val_mean_absolute_error: 0.2056\n",
      "Epoch 332/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2121 - mean_absolute_error: 0.2121\n",
      "Epoch 332: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2124 - mean_absolute_error: 0.2124 - val_loss: 0.2089 - val_mean_absolute_error: 0.2089\n",
      "Epoch 333/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 333: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2072 - mean_absolute_error: 0.2072 - val_loss: 0.2221 - val_mean_absolute_error: 0.2221\n",
      "Epoch 334/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2098 - mean_absolute_error: 0.2098\n",
      "Epoch 334: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2085 - mean_absolute_error: 0.2085 - val_loss: 0.2159 - val_mean_absolute_error: 0.2159\n",
      "Epoch 335/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2046 - mean_absolute_error: 0.2046\n",
      "Epoch 335: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2055 - mean_absolute_error: 0.2055 - val_loss: 0.2185 - val_mean_absolute_error: 0.2185\n",
      "Epoch 336/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2039 - mean_absolute_error: 0.2039\n",
      "Epoch 336: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2042 - mean_absolute_error: 0.2042 - val_loss: 0.2129 - val_mean_absolute_error: 0.2129\n",
      "Epoch 337/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2065 - mean_absolute_error: 0.2065\n",
      "Epoch 337: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2058 - mean_absolute_error: 0.2058 - val_loss: 0.2181 - val_mean_absolute_error: 0.2181\n",
      "Epoch 338/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 338: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2075 - mean_absolute_error: 0.2075 - val_loss: 0.2311 - val_mean_absolute_error: 0.2311\n",
      "Epoch 339/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2111 - mean_absolute_error: 0.2111\n",
      "Epoch 339: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2111 - mean_absolute_error: 0.2111 - val_loss: 0.2187 - val_mean_absolute_error: 0.2187\n",
      "Epoch 340/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2094 - mean_absolute_error: 0.2094\n",
      "Epoch 340: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2081 - mean_absolute_error: 0.2081 - val_loss: 0.2219 - val_mean_absolute_error: 0.2219\n",
      "Epoch 341/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2074 - mean_absolute_error: 0.2074\n",
      "Epoch 341: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2076 - mean_absolute_error: 0.2076 - val_loss: 0.2142 - val_mean_absolute_error: 0.2142\n",
      "Epoch 342/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2086 - mean_absolute_error: 0.2086\n",
      "Epoch 342: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2077 - mean_absolute_error: 0.2077 - val_loss: 0.2221 - val_mean_absolute_error: 0.2221\n",
      "Epoch 343/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2131 - mean_absolute_error: 0.2131\n",
      "Epoch 343: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2132 - mean_absolute_error: 0.2132 - val_loss: 0.2269 - val_mean_absolute_error: 0.2269\n",
      "Epoch 344/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2142 - mean_absolute_error: 0.2142\n",
      "Epoch 344: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2139 - mean_absolute_error: 0.2139 - val_loss: 0.2216 - val_mean_absolute_error: 0.2216\n",
      "Epoch 345/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2173 - mean_absolute_error: 0.2173\n",
      "Epoch 345: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2265 - val_mean_absolute_error: 0.2265\n",
      "Epoch 346/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2168 - mean_absolute_error: 0.2168\n",
      "Epoch 346: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2215 - val_mean_absolute_error: 0.2215\n",
      "Epoch 347/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2180 - mean_absolute_error: 0.2180\n",
      "Epoch 347: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2181 - mean_absolute_error: 0.2181 - val_loss: 0.2269 - val_mean_absolute_error: 0.2269\n",
      "Epoch 348/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2255 - mean_absolute_error: 0.2255\n",
      "Epoch 348: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2243 - mean_absolute_error: 0.2243 - val_loss: 0.2121 - val_mean_absolute_error: 0.2121\n",
      "Epoch 349/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2194 - mean_absolute_error: 0.2194\n",
      "Epoch 349: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2205 - mean_absolute_error: 0.2205 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 350/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2161 - mean_absolute_error: 0.2161\n",
      "Epoch 350: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2157 - mean_absolute_error: 0.2157 - val_loss: 0.2161 - val_mean_absolute_error: 0.2161\n",
      "Epoch 351/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2159 - mean_absolute_error: 0.2159\n",
      "Epoch 351: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2089 - val_mean_absolute_error: 0.2089\n",
      "Epoch 352/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2170 - mean_absolute_error: 0.2170\n",
      "Epoch 352: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2171 - mean_absolute_error: 0.2171 - val_loss: 0.2201 - val_mean_absolute_error: 0.2201\n",
      "Epoch 353/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2101 - mean_absolute_error: 0.2101\n",
      "Epoch 353: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2111 - mean_absolute_error: 0.2111 - val_loss: 0.2196 - val_mean_absolute_error: 0.2196\n",
      "Epoch 354/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2075 - mean_absolute_error: 0.2075\n",
      "Epoch 354: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2085 - mean_absolute_error: 0.2085 - val_loss: 0.2166 - val_mean_absolute_error: 0.2166\n",
      "Epoch 355/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2085 - mean_absolute_error: 0.2085\n",
      "Epoch 355: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2092 - mean_absolute_error: 0.2092 - val_loss: 0.2137 - val_mean_absolute_error: 0.2137\n",
      "Epoch 356/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2153 - mean_absolute_error: 0.2153\n",
      "Epoch 356: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2144 - mean_absolute_error: 0.2144 - val_loss: 0.2156 - val_mean_absolute_error: 0.2156\n",
      "Epoch 357/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2083 - mean_absolute_error: 0.2083\n",
      "Epoch 357: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2088 - mean_absolute_error: 0.2088 - val_loss: 0.2263 - val_mean_absolute_error: 0.2263\n",
      "Epoch 358/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2084 - mean_absolute_error: 0.2084\n",
      "Epoch 358: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2081 - mean_absolute_error: 0.2081 - val_loss: 0.2075 - val_mean_absolute_error: 0.2075\n",
      "Epoch 359/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2075 - mean_absolute_error: 0.2075\n",
      "Epoch 359: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2082 - mean_absolute_error: 0.2082 - val_loss: 0.2212 - val_mean_absolute_error: 0.2212\n",
      "Epoch 360/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2082 - mean_absolute_error: 0.2082\n",
      "Epoch 360: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2070 - mean_absolute_error: 0.2070 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 361/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2042 - mean_absolute_error: 0.2042\n",
      "Epoch 361: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2043 - mean_absolute_error: 0.2043 - val_loss: 0.2144 - val_mean_absolute_error: 0.2144\n",
      "Epoch 362/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2065 - mean_absolute_error: 0.2065\n",
      "Epoch 362: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.2129 - val_mean_absolute_error: 0.2129\n",
      "Epoch 363/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2079 - mean_absolute_error: 0.2079\n",
      "Epoch 363: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2066 - mean_absolute_error: 0.2066 - val_loss: 0.2088 - val_mean_absolute_error: 0.2088\n",
      "Epoch 364/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2084 - mean_absolute_error: 0.2084\n",
      "Epoch 364: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2071 - mean_absolute_error: 0.2071 - val_loss: 0.2150 - val_mean_absolute_error: 0.2150\n",
      "Epoch 365/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2061 - mean_absolute_error: 0.2061\n",
      "Epoch 365: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2068 - mean_absolute_error: 0.2068 - val_loss: 0.2083 - val_mean_absolute_error: 0.2083\n",
      "Epoch 366/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2042 - mean_absolute_error: 0.2042\n",
      "Epoch 366: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2055 - mean_absolute_error: 0.2055 - val_loss: 0.2121 - val_mean_absolute_error: 0.2121\n",
      "Epoch 367/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2024 - mean_absolute_error: 0.2024\n",
      "Epoch 367: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2026 - mean_absolute_error: 0.2026 - val_loss: 0.2205 - val_mean_absolute_error: 0.2205\n",
      "Epoch 368/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2048 - mean_absolute_error: 0.2048\n",
      "Epoch 368: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2033 - mean_absolute_error: 0.2033 - val_loss: 0.2078 - val_mean_absolute_error: 0.2078\n",
      "Epoch 369/500\n",
      "140/153 [==========================>...] - ETA: 0s - loss: 0.2054 - mean_absolute_error: 0.2054\n",
      "Epoch 369: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2057 - mean_absolute_error: 0.2057 - val_loss: 0.2054 - val_mean_absolute_error: 0.2054\n",
      "Epoch 370/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2049 - mean_absolute_error: 0.2049\n",
      "Epoch 370: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2036 - mean_absolute_error: 0.2036 - val_loss: 0.2104 - val_mean_absolute_error: 0.2104\n",
      "Epoch 371/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2045 - mean_absolute_error: 0.2045\n",
      "Epoch 371: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2048 - mean_absolute_error: 0.2048 - val_loss: 0.2114 - val_mean_absolute_error: 0.2114\n",
      "Epoch 372/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2050 - mean_absolute_error: 0.2050\n",
      "Epoch 372: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2050 - mean_absolute_error: 0.2050 - val_loss: 0.2176 - val_mean_absolute_error: 0.2176\n",
      "Epoch 373/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2066 - mean_absolute_error: 0.2066\n",
      "Epoch 373: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2132 - val_mean_absolute_error: 0.2132\n",
      "Epoch 374/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2073 - mean_absolute_error: 0.2073\n",
      "Epoch 374: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2062 - mean_absolute_error: 0.2062 - val_loss: 0.2127 - val_mean_absolute_error: 0.2127\n",
      "Epoch 375/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2026 - mean_absolute_error: 0.2026\n",
      "Epoch 375: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2039 - mean_absolute_error: 0.2039 - val_loss: 0.2095 - val_mean_absolute_error: 0.2095\n",
      "Epoch 376/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2068 - mean_absolute_error: 0.2068\n",
      "Epoch 376: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2068 - mean_absolute_error: 0.2068 - val_loss: 0.2204 - val_mean_absolute_error: 0.2204\n",
      "Epoch 377/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 377: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2076 - mean_absolute_error: 0.2076 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 378/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2026 - mean_absolute_error: 0.2026\n",
      "Epoch 378: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2054 - mean_absolute_error: 0.2054 - val_loss: 0.2171 - val_mean_absolute_error: 0.2171\n",
      "Epoch 379/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2210 - mean_absolute_error: 0.2210\n",
      "Epoch 379: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2196 - mean_absolute_error: 0.2196 - val_loss: 0.2097 - val_mean_absolute_error: 0.2097\n",
      "Epoch 380/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2116 - mean_absolute_error: 0.2116\n",
      "Epoch 380: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2116 - mean_absolute_error: 0.2116 - val_loss: 0.2075 - val_mean_absolute_error: 0.2075\n",
      "Epoch 381/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2152 - mean_absolute_error: 0.2152\n",
      "Epoch 381: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2133 - mean_absolute_error: 0.2133 - val_loss: 0.2144 - val_mean_absolute_error: 0.2144\n",
      "Epoch 382/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2129 - mean_absolute_error: 0.2129\n",
      "Epoch 382: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2127 - mean_absolute_error: 0.2127 - val_loss: 0.2215 - val_mean_absolute_error: 0.2215\n",
      "Epoch 383/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2120 - mean_absolute_error: 0.2120\n",
      "Epoch 383: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2134 - mean_absolute_error: 0.2134 - val_loss: 0.2157 - val_mean_absolute_error: 0.2157\n",
      "Epoch 384/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2114 - mean_absolute_error: 0.2114\n",
      "Epoch 384: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2104 - mean_absolute_error: 0.2104 - val_loss: 0.2057 - val_mean_absolute_error: 0.2057\n",
      "Epoch 385/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2111 - mean_absolute_error: 0.2111\n",
      "Epoch 385: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2113 - mean_absolute_error: 0.2113 - val_loss: 0.2269 - val_mean_absolute_error: 0.2269\n",
      "Epoch 386/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2140 - mean_absolute_error: 0.2140\n",
      "Epoch 386: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2126 - mean_absolute_error: 0.2126 - val_loss: 0.2251 - val_mean_absolute_error: 0.2251\n",
      "Epoch 387/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2169 - mean_absolute_error: 0.2169\n",
      "Epoch 387: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2376 - val_mean_absolute_error: 0.2376\n",
      "Epoch 388/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2172 - mean_absolute_error: 0.2172\n",
      "Epoch 388: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2351 - val_mean_absolute_error: 0.2351\n",
      "Epoch 389/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2211 - mean_absolute_error: 0.2211\n",
      "Epoch 389: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2200 - mean_absolute_error: 0.2200 - val_loss: 0.2380 - val_mean_absolute_error: 0.2380\n",
      "Epoch 390/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2264 - mean_absolute_error: 0.2264\n",
      "Epoch 390: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2278 - mean_absolute_error: 0.2278 - val_loss: 0.2599 - val_mean_absolute_error: 0.2599\n",
      "Epoch 391/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2259 - mean_absolute_error: 0.2259\n",
      "Epoch 391: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2254 - mean_absolute_error: 0.2254 - val_loss: 0.2391 - val_mean_absolute_error: 0.2391\n",
      "Epoch 392/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2198 - mean_absolute_error: 0.2198\n",
      "Epoch 392: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2186 - mean_absolute_error: 0.2186 - val_loss: 0.2328 - val_mean_absolute_error: 0.2328\n",
      "Epoch 393/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2126 - mean_absolute_error: 0.2126\n",
      "Epoch 393: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2138 - mean_absolute_error: 0.2138 - val_loss: 0.2068 - val_mean_absolute_error: 0.2068\n",
      "Epoch 394/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2090 - mean_absolute_error: 0.2090\n",
      "Epoch 394: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2077 - mean_absolute_error: 0.2077 - val_loss: 0.2069 - val_mean_absolute_error: 0.2069\n",
      "Epoch 395/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2071 - mean_absolute_error: 0.2071\n",
      "Epoch 395: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2080 - val_mean_absolute_error: 0.2080\n",
      "Epoch 396/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2070 - mean_absolute_error: 0.2070\n",
      "Epoch 396: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2072 - mean_absolute_error: 0.2072 - val_loss: 0.2121 - val_mean_absolute_error: 0.2121\n",
      "Epoch 397/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2104 - mean_absolute_error: 0.2104\n",
      "Epoch 397: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2083 - mean_absolute_error: 0.2083 - val_loss: 0.2096 - val_mean_absolute_error: 0.2096\n",
      "Epoch 398/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2048 - mean_absolute_error: 0.2048\n",
      "Epoch 398: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2047 - mean_absolute_error: 0.2047 - val_loss: 0.2233 - val_mean_absolute_error: 0.2233\n",
      "Epoch 399/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2048 - mean_absolute_error: 0.2048\n",
      "Epoch 399: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2050 - mean_absolute_error: 0.2050 - val_loss: 0.2272 - val_mean_absolute_error: 0.2272\n",
      "Epoch 400/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2067 - mean_absolute_error: 0.2067\n",
      "Epoch 400: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2066 - mean_absolute_error: 0.2066 - val_loss: 0.2122 - val_mean_absolute_error: 0.2122\n",
      "Epoch 401/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2054 - mean_absolute_error: 0.2054\n",
      "Epoch 401: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2033 - mean_absolute_error: 0.2033 - val_loss: 0.2090 - val_mean_absolute_error: 0.2090\n",
      "Epoch 402/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2048 - mean_absolute_error: 0.2048\n",
      "Epoch 402: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2048 - mean_absolute_error: 0.2048 - val_loss: 0.2180 - val_mean_absolute_error: 0.2180\n",
      "Epoch 403/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2073 - mean_absolute_error: 0.2073\n",
      "Epoch 403: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2051 - mean_absolute_error: 0.2051 - val_loss: 0.2172 - val_mean_absolute_error: 0.2172\n",
      "Epoch 404/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2100 - mean_absolute_error: 0.2100\n",
      "Epoch 404: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2092 - mean_absolute_error: 0.2092 - val_loss: 0.2168 - val_mean_absolute_error: 0.2168\n",
      "Epoch 405/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2074 - mean_absolute_error: 0.2074\n",
      "Epoch 405: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2089 - mean_absolute_error: 0.2089 - val_loss: 0.2210 - val_mean_absolute_error: 0.2210\n",
      "Epoch 406/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2124 - mean_absolute_error: 0.2124\n",
      "Epoch 406: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2119 - mean_absolute_error: 0.2119 - val_loss: 0.2164 - val_mean_absolute_error: 0.2164\n",
      "Epoch 407/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2103 - mean_absolute_error: 0.2103\n",
      "Epoch 407: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2100 - mean_absolute_error: 0.2100 - val_loss: 0.2226 - val_mean_absolute_error: 0.2226\n",
      "Epoch 408/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2242 - mean_absolute_error: 0.2242\n",
      "Epoch 408: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2229 - mean_absolute_error: 0.2229 - val_loss: 0.2157 - val_mean_absolute_error: 0.2157\n",
      "Epoch 409/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2240 - mean_absolute_error: 0.2240\n",
      "Epoch 409: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2246 - mean_absolute_error: 0.2246 - val_loss: 0.2222 - val_mean_absolute_error: 0.2222\n",
      "Epoch 410/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2185 - mean_absolute_error: 0.2185\n",
      "Epoch 410: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2183 - mean_absolute_error: 0.2183 - val_loss: 0.2212 - val_mean_absolute_error: 0.2212\n",
      "Epoch 411/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2194 - mean_absolute_error: 0.2194\n",
      "Epoch 411: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2208 - val_mean_absolute_error: 0.2208\n",
      "Epoch 412/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2192 - mean_absolute_error: 0.2192\n",
      "Epoch 412: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2200 - mean_absolute_error: 0.2200 - val_loss: 0.2241 - val_mean_absolute_error: 0.2241\n",
      "Epoch 413/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2163 - mean_absolute_error: 0.2163\n",
      "Epoch 413: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2162 - mean_absolute_error: 0.2162 - val_loss: 0.2149 - val_mean_absolute_error: 0.2149\n",
      "Epoch 414/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2129 - mean_absolute_error: 0.2129\n",
      "Epoch 414: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2129 - mean_absolute_error: 0.2129 - val_loss: 0.2142 - val_mean_absolute_error: 0.2142\n",
      "Epoch 415/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2059 - mean_absolute_error: 0.2059\n",
      "Epoch 415: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2081 - mean_absolute_error: 0.2081 - val_loss: 0.2093 - val_mean_absolute_error: 0.2093\n",
      "Epoch 416/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2065 - mean_absolute_error: 0.2065\n",
      "Epoch 416: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2091 - mean_absolute_error: 0.2091 - val_loss: 0.2205 - val_mean_absolute_error: 0.2205\n",
      "Epoch 417/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2109 - mean_absolute_error: 0.2109\n",
      "Epoch 417: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2109 - mean_absolute_error: 0.2109 - val_loss: 0.2161 - val_mean_absolute_error: 0.2161\n",
      "Epoch 418/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2108 - mean_absolute_error: 0.2108\n",
      "Epoch 418: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2110 - mean_absolute_error: 0.2110 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 419/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2117 - mean_absolute_error: 0.2117\n",
      "Epoch 419: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2101 - mean_absolute_error: 0.2101 - val_loss: 0.2112 - val_mean_absolute_error: 0.2112\n",
      "Epoch 420/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2141 - mean_absolute_error: 0.2141\n",
      "Epoch 420: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2148 - mean_absolute_error: 0.2148 - val_loss: 0.2121 - val_mean_absolute_error: 0.2121\n",
      "Epoch 421/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2106 - mean_absolute_error: 0.2106\n",
      "Epoch 421: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2105 - mean_absolute_error: 0.2105 - val_loss: 0.2139 - val_mean_absolute_error: 0.2139\n",
      "Epoch 422/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2090 - mean_absolute_error: 0.2090\n",
      "Epoch 422: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2095 - mean_absolute_error: 0.2095 - val_loss: 0.2154 - val_mean_absolute_error: 0.2154\n",
      "Epoch 423/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2152 - mean_absolute_error: 0.2152\n",
      "Epoch 423: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2152 - mean_absolute_error: 0.2152 - val_loss: 0.2121 - val_mean_absolute_error: 0.2121\n",
      "Epoch 424/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2137 - mean_absolute_error: 0.2137\n",
      "Epoch 424: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2115 - mean_absolute_error: 0.2115 - val_loss: 0.2127 - val_mean_absolute_error: 0.2127\n",
      "Epoch 425/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2100 - mean_absolute_error: 0.2100\n",
      "Epoch 425: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2092 - mean_absolute_error: 0.2092 - val_loss: 0.2135 - val_mean_absolute_error: 0.2135\n",
      "Epoch 426/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2104 - mean_absolute_error: 0.2104\n",
      "Epoch 426: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2109 - mean_absolute_error: 0.2109 - val_loss: 0.2116 - val_mean_absolute_error: 0.2116\n",
      "Epoch 427/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2138 - mean_absolute_error: 0.2138\n",
      "Epoch 427: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2136 - mean_absolute_error: 0.2136 - val_loss: 0.2167 - val_mean_absolute_error: 0.2167\n",
      "Epoch 428/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2110 - mean_absolute_error: 0.2110\n",
      "Epoch 428: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2111 - mean_absolute_error: 0.2111 - val_loss: 0.2104 - val_mean_absolute_error: 0.2104\n",
      "Epoch 429/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2150 - mean_absolute_error: 0.2150\n",
      "Epoch 429: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2144 - mean_absolute_error: 0.2144 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 430/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2091 - mean_absolute_error: 0.2091\n",
      "Epoch 430: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2086 - mean_absolute_error: 0.2086 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
      "Epoch 431/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2100 - mean_absolute_error: 0.2100\n",
      "Epoch 431: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2098 - mean_absolute_error: 0.2098 - val_loss: 0.2211 - val_mean_absolute_error: 0.2211\n",
      "Epoch 432/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2080 - mean_absolute_error: 0.2080\n",
      "Epoch 432: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2077 - mean_absolute_error: 0.2077 - val_loss: 0.2098 - val_mean_absolute_error: 0.2098\n",
      "Epoch 433/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2086 - mean_absolute_error: 0.2086\n",
      "Epoch 433: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2072 - mean_absolute_error: 0.2072 - val_loss: 0.2270 - val_mean_absolute_error: 0.2270\n",
      "Epoch 434/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2114 - mean_absolute_error: 0.2114\n",
      "Epoch 434: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2121 - mean_absolute_error: 0.2121 - val_loss: 0.2204 - val_mean_absolute_error: 0.2204\n",
      "Epoch 435/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
      "Epoch 435: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2092 - mean_absolute_error: 0.2092 - val_loss: 0.2231 - val_mean_absolute_error: 0.2231\n",
      "Epoch 436/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2071 - mean_absolute_error: 0.2071\n",
      "Epoch 436: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2080 - mean_absolute_error: 0.2080 - val_loss: 0.2167 - val_mean_absolute_error: 0.2167\n",
      "Epoch 437/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2044 - mean_absolute_error: 0.2044\n",
      "Epoch 437: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2049 - mean_absolute_error: 0.2049 - val_loss: 0.2133 - val_mean_absolute_error: 0.2133\n",
      "Epoch 438/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2043 - mean_absolute_error: 0.2043\n",
      "Epoch 438: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2040 - mean_absolute_error: 0.2040 - val_loss: 0.2174 - val_mean_absolute_error: 0.2174\n",
      "Epoch 439/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2079 - mean_absolute_error: 0.2079\n",
      "Epoch 439: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2116 - val_mean_absolute_error: 0.2116\n",
      "Epoch 440/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2047 - mean_absolute_error: 0.2047\n",
      "Epoch 440: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2037 - mean_absolute_error: 0.2037 - val_loss: 0.2108 - val_mean_absolute_error: 0.2108\n",
      "Epoch 441/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2027 - mean_absolute_error: 0.2027\n",
      "Epoch 441: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2027 - mean_absolute_error: 0.2027 - val_loss: 0.2104 - val_mean_absolute_error: 0.2104\n",
      "Epoch 442/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2041 - mean_absolute_error: 0.2041\n",
      "Epoch 442: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2027 - mean_absolute_error: 0.2027 - val_loss: 0.2115 - val_mean_absolute_error: 0.2115\n",
      "Epoch 443/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2061 - mean_absolute_error: 0.2061\n",
      "Epoch 443: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2055 - mean_absolute_error: 0.2055 - val_loss: 0.2114 - val_mean_absolute_error: 0.2114\n",
      "Epoch 444/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2082 - mean_absolute_error: 0.2082\n",
      "Epoch 444: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2079 - mean_absolute_error: 0.2079 - val_loss: 0.2114 - val_mean_absolute_error: 0.2114\n",
      "Epoch 445/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2033 - mean_absolute_error: 0.2033\n",
      "Epoch 445: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2031 - mean_absolute_error: 0.2031 - val_loss: 0.2107 - val_mean_absolute_error: 0.2107\n",
      "Epoch 446/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2022 - mean_absolute_error: 0.2022\n",
      "Epoch 446: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2021 - mean_absolute_error: 0.2021 - val_loss: 0.2141 - val_mean_absolute_error: 0.2141\n",
      "Epoch 447/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2142 - mean_absolute_error: 0.2142\n",
      "Epoch 447: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2141 - mean_absolute_error: 0.2141 - val_loss: 0.2099 - val_mean_absolute_error: 0.2099\n",
      "Epoch 448/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2167 - mean_absolute_error: 0.2167\n",
      "Epoch 448: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2111 - val_mean_absolute_error: 0.2111\n",
      "Epoch 449/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2160 - mean_absolute_error: 0.2160\n",
      "Epoch 449: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2151 - mean_absolute_error: 0.2151 - val_loss: 0.2203 - val_mean_absolute_error: 0.2203\n",
      "Epoch 450/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2208 - mean_absolute_error: 0.2208\n",
      "Epoch 450: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2199 - mean_absolute_error: 0.2199 - val_loss: 0.2296 - val_mean_absolute_error: 0.2296\n",
      "Epoch 451/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2181 - mean_absolute_error: 0.2181\n",
      "Epoch 451: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2173 - mean_absolute_error: 0.2173 - val_loss: 0.2097 - val_mean_absolute_error: 0.2097\n",
      "Epoch 452/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2073 - mean_absolute_error: 0.2073\n",
      "Epoch 452: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2068 - mean_absolute_error: 0.2068 - val_loss: 0.2401 - val_mean_absolute_error: 0.2401\n",
      "Epoch 453/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2261 - mean_absolute_error: 0.2261\n",
      "Epoch 453: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2261 - mean_absolute_error: 0.2261 - val_loss: 0.2338 - val_mean_absolute_error: 0.2338\n",
      "Epoch 454/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2282 - mean_absolute_error: 0.2282\n",
      "Epoch 454: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2280 - mean_absolute_error: 0.2280 - val_loss: 0.2328 - val_mean_absolute_error: 0.2328\n",
      "Epoch 455/500\n",
      "140/153 [==========================>...] - ETA: 0s - loss: 0.2292 - mean_absolute_error: 0.2292\n",
      "Epoch 455: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2283 - mean_absolute_error: 0.2283 - val_loss: 0.2327 - val_mean_absolute_error: 0.2327\n",
      "Epoch 456/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2249 - mean_absolute_error: 0.2249\n",
      "Epoch 456: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2259 - mean_absolute_error: 0.2259 - val_loss: 0.2314 - val_mean_absolute_error: 0.2314\n",
      "Epoch 457/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2211 - mean_absolute_error: 0.2211\n",
      "Epoch 457: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2241 - mean_absolute_error: 0.2241 - val_loss: 0.2333 - val_mean_absolute_error: 0.2333\n",
      "Epoch 458/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2271 - mean_absolute_error: 0.2271\n",
      "Epoch 458: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2283 - mean_absolute_error: 0.2283 - val_loss: 0.2441 - val_mean_absolute_error: 0.2441\n",
      "Epoch 459/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2242 - mean_absolute_error: 0.2242\n",
      "Epoch 459: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2262 - mean_absolute_error: 0.2262 - val_loss: 0.2439 - val_mean_absolute_error: 0.2439\n",
      "Epoch 460/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2390 - mean_absolute_error: 0.2390\n",
      "Epoch 460: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2380 - mean_absolute_error: 0.2380 - val_loss: 0.2421 - val_mean_absolute_error: 0.2421\n",
      "Epoch 461/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2378 - mean_absolute_error: 0.2378\n",
      "Epoch 461: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2377 - val_mean_absolute_error: 0.2377\n",
      "Epoch 462/500\n",
      "148/153 [============================>.] - ETA: 0s - loss: 0.2410 - mean_absolute_error: 0.2410\n",
      "Epoch 462: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2405 - mean_absolute_error: 0.2405 - val_loss: 0.2369 - val_mean_absolute_error: 0.2369\n",
      "Epoch 463/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2345 - mean_absolute_error: 0.2345\n",
      "Epoch 463: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2357 - mean_absolute_error: 0.2357 - val_loss: 0.2366 - val_mean_absolute_error: 0.2366\n",
      "Epoch 464/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2355 - mean_absolute_error: 0.2355\n",
      "Epoch 464: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2335 - mean_absolute_error: 0.2335 - val_loss: 0.2385 - val_mean_absolute_error: 0.2385\n",
      "Epoch 465/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2349 - mean_absolute_error: 0.2349\n",
      "Epoch 465: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2340 - mean_absolute_error: 0.2340 - val_loss: 0.2354 - val_mean_absolute_error: 0.2354\n",
      "Epoch 466/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2336 - mean_absolute_error: 0.2336\n",
      "Epoch 466: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2354 - val_mean_absolute_error: 0.2354\n",
      "Epoch 467/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2274 - mean_absolute_error: 0.2274\n",
      "Epoch 467: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2282 - mean_absolute_error: 0.2282 - val_loss: 0.2306 - val_mean_absolute_error: 0.2306\n",
      "Epoch 468/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2254 - mean_absolute_error: 0.2254\n",
      "Epoch 468: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2273 - mean_absolute_error: 0.2273 - val_loss: 0.2265 - val_mean_absolute_error: 0.2265\n",
      "Epoch 469/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2233 - mean_absolute_error: 0.2233\n",
      "Epoch 469: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2232 - mean_absolute_error: 0.2232 - val_loss: 0.2310 - val_mean_absolute_error: 0.2310\n",
      "Epoch 470/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2194 - mean_absolute_error: 0.2194\n",
      "Epoch 470: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2191 - mean_absolute_error: 0.2191 - val_loss: 0.2066 - val_mean_absolute_error: 0.2066\n",
      "Epoch 471/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.2081\n",
      "Epoch 471: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2088 - mean_absolute_error: 0.2088 - val_loss: 0.2186 - val_mean_absolute_error: 0.2186\n",
      "Epoch 472/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2128 - mean_absolute_error: 0.2128\n",
      "Epoch 472: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2121 - mean_absolute_error: 0.2121 - val_loss: 0.2103 - val_mean_absolute_error: 0.2103\n",
      "Epoch 473/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2138 - mean_absolute_error: 0.2138\n",
      "Epoch 473: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2146 - mean_absolute_error: 0.2146 - val_loss: 0.2107 - val_mean_absolute_error: 0.2107\n",
      "Epoch 474/500\n",
      "142/153 [==========================>...] - ETA: 0s - loss: 0.2148 - mean_absolute_error: 0.2148\n",
      "Epoch 474: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2151 - mean_absolute_error: 0.2151 - val_loss: 0.2178 - val_mean_absolute_error: 0.2178\n",
      "Epoch 475/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2142 - mean_absolute_error: 0.2142\n",
      "Epoch 475: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2148 - mean_absolute_error: 0.2148 - val_loss: 0.2107 - val_mean_absolute_error: 0.2107\n",
      "Epoch 476/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2115 - mean_absolute_error: 0.2115\n",
      "Epoch 476: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2115 - mean_absolute_error: 0.2115 - val_loss: 0.2153 - val_mean_absolute_error: 0.2153\n",
      "Epoch 477/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2101 - mean_absolute_error: 0.2101\n",
      "Epoch 477: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2109 - mean_absolute_error: 0.2109 - val_loss: 0.2173 - val_mean_absolute_error: 0.2173\n",
      "Epoch 478/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2168 - mean_absolute_error: 0.2168\n",
      "Epoch 478: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2146 - val_mean_absolute_error: 0.2146\n",
      "Epoch 479/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2093 - mean_absolute_error: 0.2093\n",
      "Epoch 479: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2093 - mean_absolute_error: 0.2093 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
      "Epoch 480/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2114 - mean_absolute_error: 0.2114\n",
      "Epoch 480: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2102 - mean_absolute_error: 0.2102 - val_loss: 0.2126 - val_mean_absolute_error: 0.2126\n",
      "Epoch 481/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2053 - mean_absolute_error: 0.2053\n",
      "Epoch 481: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2035 - mean_absolute_error: 0.2035 - val_loss: 0.2093 - val_mean_absolute_error: 0.2093\n",
      "Epoch 482/500\n",
      "144/153 [===========================>..] - ETA: 0s - loss: 0.2086 - mean_absolute_error: 0.2086\n",
      "Epoch 482: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2085 - mean_absolute_error: 0.2085 - val_loss: 0.2165 - val_mean_absolute_error: 0.2165\n",
      "Epoch 483/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2060 - mean_absolute_error: 0.2060\n",
      "Epoch 483: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2056 - mean_absolute_error: 0.2056 - val_loss: 0.2099 - val_mean_absolute_error: 0.2099\n",
      "Epoch 484/500\n",
      "150/153 [============================>.] - ETA: 0s - loss: 0.2059 - mean_absolute_error: 0.2059\n",
      "Epoch 484: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.2049 - mean_absolute_error: 0.2049 - val_loss: 0.2150 - val_mean_absolute_error: 0.2150\n",
      "Epoch 485/500\n",
      "151/153 [============================>.] - ETA: 0s - loss: 0.2036 - mean_absolute_error: 0.2036\n",
      "Epoch 485: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2042 - mean_absolute_error: 0.2042 - val_loss: 0.2140 - val_mean_absolute_error: 0.2140\n",
      "Epoch 486/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2056 - mean_absolute_error: 0.2056\n",
      "Epoch 486: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.2083 - val_mean_absolute_error: 0.2083\n",
      "Epoch 487/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.1950 - mean_absolute_error: 0.1950\n",
      "Epoch 487: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.1982 - mean_absolute_error: 0.1982 - val_loss: 0.2106 - val_mean_absolute_error: 0.2106\n",
      "Epoch 488/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2050 - mean_absolute_error: 0.2050\n",
      "Epoch 488: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2044 - mean_absolute_error: 0.2044 - val_loss: 0.2153 - val_mean_absolute_error: 0.2153\n",
      "Epoch 489/500\n",
      "141/153 [==========================>...] - ETA: 0s - loss: 0.2070 - mean_absolute_error: 0.2070\n",
      "Epoch 489: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2063 - mean_absolute_error: 0.2063 - val_loss: 0.2067 - val_mean_absolute_error: 0.2067\n",
      "Epoch 490/500\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2039 - mean_absolute_error: 0.2039\n",
      "Epoch 490: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2041 - mean_absolute_error: 0.2041 - val_loss: 0.2152 - val_mean_absolute_error: 0.2152\n",
      "Epoch 491/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2063 - mean_absolute_error: 0.2063\n",
      "Epoch 491: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2057 - mean_absolute_error: 0.2057 - val_loss: 0.2077 - val_mean_absolute_error: 0.2077\n",
      "Epoch 492/500\n",
      "143/153 [===========================>..] - ETA: 0s - loss: 0.2010 - mean_absolute_error: 0.2010\n",
      "Epoch 492: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2025 - mean_absolute_error: 0.2025 - val_loss: 0.2134 - val_mean_absolute_error: 0.2134\n",
      "Epoch 493/500\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2040 - mean_absolute_error: 0.2040\n",
      "Epoch 493: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.2040 - mean_absolute_error: 0.2040 - val_loss: 0.2192 - val_mean_absolute_error: 0.2192\n",
      "Epoch 494/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2087 - mean_absolute_error: 0.2087\n",
      "Epoch 494: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2090 - mean_absolute_error: 0.2090 - val_loss: 0.2100 - val_mean_absolute_error: 0.2100\n",
      "Epoch 495/500\n",
      "140/153 [==========================>...] - ETA: 0s - loss: 0.2105 - mean_absolute_error: 0.2105\n",
      "Epoch 495: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2091 - mean_absolute_error: 0.2091 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
      "Epoch 496/500\n",
      "146/153 [===========================>..] - ETA: 0s - loss: 0.2099 - mean_absolute_error: 0.2099\n",
      "Epoch 496: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2103 - mean_absolute_error: 0.2103 - val_loss: 0.2114 - val_mean_absolute_error: 0.2114\n",
      "Epoch 497/500\n",
      "147/153 [===========================>..] - ETA: 0s - loss: 0.2075 - mean_absolute_error: 0.2075\n",
      "Epoch 497: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2069 - mean_absolute_error: 0.2069 - val_loss: 0.2149 - val_mean_absolute_error: 0.2149\n",
      "Epoch 498/500\n",
      "145/153 [===========================>..] - ETA: 0s - loss: 0.2128 - mean_absolute_error: 0.2128\n",
      "Epoch 498: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2127 - mean_absolute_error: 0.2127 - val_loss: 0.2118 - val_mean_absolute_error: 0.2118\n",
      "Epoch 499/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2077 - mean_absolute_error: 0.2077\n",
      "Epoch 499: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2083 - mean_absolute_error: 0.2083 - val_loss: 0.2096 - val_mean_absolute_error: 0.2096\n",
      "Epoch 500/500\n",
      "149/153 [============================>.] - ETA: 0s - loss: 0.2091 - mean_absolute_error: 0.2091\n",
      "Epoch 500: val_loss did not improve from 0.20318\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.2086 - mean_absolute_error: 0.2086 - val_loss: 0.2144 - val_mean_absolute_error: 0.2144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ea6ad426790>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c0ef39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:21:29.590128Z",
     "iopub.status.busy": "2023-08-05T15:21:29.588937Z",
     "iopub.status.idle": "2023-08-05T15:21:29.626345Z",
     "shell.execute_reply": "2023-08-05T15:21:29.624809Z"
    },
    "papermill": {
     "duration": 0.829708,
     "end_time": "2023-08-05T15:21:29.629389",
     "exception": false,
     "start_time": "2023-08-05T15:21:28.799681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wights_file = 'best.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703491b",
   "metadata": {
    "id": "DAtAR0vkmnje",
    "papermill": {
     "duration": 0.718227,
     "end_time": "2023-08-05T15:21:31.074188",
     "exception": false,
     "start_time": "2023-08-05T15:21:30.355961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b64655c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:21:32.577805Z",
     "iopub.status.busy": "2023-08-05T15:21:32.577323Z",
     "iopub.status.idle": "2023-08-05T15:21:33.213627Z",
     "shell.execute_reply": "2023-08-05T15:21:33.212464Z"
    },
    "id": "KHBRzBD9mnjf",
    "papermill": {
     "duration": 1.361999,
     "end_time": "2023-08-05T15:21:33.216525",
     "exception": false,
     "start_time": "2023-08-05T15:21:31.854526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0018_01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0019_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0021_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0023_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId  Transported\n",
       "0     0013_01         True\n",
       "1     0018_01        False\n",
       "2     0019_01         True\n",
       "3     0021_01         True\n",
       "4     0023_01         True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "df = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n",
    "submission_id = df.PassengerId\n",
    "\n",
    "df = df.drop(['Name'], axis=1)\n",
    "df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].fillna(value=0)\n",
    "df.isnull().sum().sort_values(ascending=False)\n",
    "df.dropna()\n",
    "df[[\"Deck\", \"Cabin_num\", \"Side\"]] = df[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "df[\"HomePlanet\"] = df[\"HomePlanet\"].astype('category')\n",
    "df[\"Destination\"] = df[\"Destination\"].astype('category')\n",
    "df[\"Deck\"] = df[\"Deck\"].astype('category')\n",
    "df[\"Side\"] = df[\"Side\"].astype('category')\n",
    "df[\"hp_cat\"] = df[\"HomePlanet\"].cat.codes\n",
    "df[\"dest_cat\"] = df[\"Destination\"].cat.codes\n",
    "df[\"deck_cat\"] = df[\"Deck\"].cat.codes\n",
    "df[\"side_cat\"] = df[\"Side\"].cat.codes\n",
    "df['VIP'] = df['VIP'].astype(int)\n",
    "df['CryoSleep'] = df['CryoSleep'].astype(int)\n",
    "df=df.drop(['Cabin_num','HomePlanet', 'Cabin', 'Destination', 'Deck', 'Side'], axis=1)\n",
    "df = df[['hp_cat', 'CryoSleep', 'dest_cat', 'VIP', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'deck_cat', 'side_cat']]\n",
    "# Get the predictions for testdata\n",
    "# hp_cat\tCryoSleep\tdest_cat\tVIP\tFoodCourt\tShoppingMall\tSpa\tVRDeck\tdeck_cat\tside_cat\n",
    "predictions =  NN_model.predict(df)\n",
    "# print(predictions)\n",
    "n_predictions = (predictions > 0.5).astype(bool)\n",
    "output = pd.DataFrame({'PassengerId': submission_id,\n",
    "                       'Transported': n_predictions.squeeze()})\n",
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12eede3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T15:21:34.756893Z",
     "iopub.status.busy": "2023-08-05T15:21:34.755552Z",
     "iopub.status.idle": "2023-08-05T15:21:34.801763Z",
     "shell.execute_reply": "2023-08-05T15:21:34.800403Z"
    },
    "id": "OZuB6CdUmnjf",
    "papermill": {
     "duration": 0.864798,
     "end_time": "2023-08-05T15:21:34.804605",
     "exception": false,
     "start_time": "2023-08-05T15:21:33.939807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0018_01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0019_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0021_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0023_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId  Transported\n",
       "0     0013_01         True\n",
       "1     0018_01        False\n",
       "2     0019_01         True\n",
       "3     0021_01         True\n",
       "4     0023_01         True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_df = pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')\n",
    "sample_submission_df['Transported'] = n_predictions\n",
    "sample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "sample_submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 419.951528,
   "end_time": "2023-08-05T15:21:39.112821",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-05T15:14:39.161293",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
